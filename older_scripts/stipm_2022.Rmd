---
title: "2022 Willapa Bay natural coho forecast"
author: "Dan.Auerbach@dfw.wa.gov, Thomas.Buehrens@dfw.wa.gov, Neala.Kendall@dfw.wa.gov" 
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
output: 
  wdfwTemplates::wdfw_html_format

---

# Summary

This script presents preseason forecasts for Willapa Bay (WB) natural coho, generated by the Washington Department of Fish and Wildlife (WDFW). The underlying datasets depend on the valuable contributions of many technical staff from Washington's salmon comanagers. In particular, Barb McClellan, Lyle Jennings and Jody Pope have provided return estimates in the rivers of Willapa Bay that form a cornerstone of the analysis.

The work was described for a [2021 SSC review](https://www.pcouncil.org/documents/2021/10/f-1-attachment-3-a-proposed-forecast-methodology-for-natural-origin-willapa-bay-coho-o-kisutch-electronic-only.pdf/), and it is conceptually grounded in [DeFilippo et al.'s 2021 publication](https://www.sciencedirect.com/science/article/pii/S0165783621001429) "Improving short-term recruitment forecasts for coho salmon using a spatiotemporal integrated population model". This approach applies forecast methods that quantify uncertainty, and thereby supporting fishery managers' evaluation of the relative risks of alternative decisions.

Much of the script is unchanged from the Oct 2021 version. Sections 4.2 (one-ahead fits) and 5 (results) were not re-executed, but were left in anticipation of the 2023 forecast, at which time they can be revisited. The final forecast values derive from the section 6 "Full dataset", drawing on the "Data/2022 updates" and the "Model fitting/stan functions" subsections. Further details are noted below.

# Setup

The following code readies the R environment (`r sessionInfo()$R.version$version.string`), calling necessary packages and loading an initial dataset.

```{r setup, results = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, warning = FALSE, message = FALSE, fig.width = 9, fig.height = 9)

library("tidyverse")
library("gt")
library("odbc"); library("DBI")
library("rstan")
options(mc.cores = 4)
rstan_options(auto_write = TRUE)
theme_set(theme_light())
library(tidybayes)

fp <- list(
  data_fram = "data/postseason_fram_spwn_hvst_ests.csv",
  data_cwt = "data/rmis_cwts.csv",
  data_smolt = "data/smolt_outmigrants.csv",
  data_full = "data/fram_cwt_smolt_fulljoin.csv"
  )

#combined full dataset
coho_data_tbl <- readr::read_csv(fp$data_full)

# ## dropped FRAM StockID 13 and 43, original pop_ids 10 and 24
# # fram_stocks <- c(105,93,89,45,51,55,75,81,61,23,149,63,107,115,111,157,97,135,153,101,69,1,85,139,131,127,145,11,17,59,35,29,117,161)
# 
# pop_meta <- readxl::read_excel("pop_meta.xlsx") |>
#   dplyr::select(pop_id:hat)
# #deleted and reindexed pop in file, no longer need: |> dplyr::filter(StockID != 13, StockID != 43)

sf_coord <- coho_data_tbl |> 
  distinct(StockID, StockLongName, pop_id, pop, lon, lat, hab_km) |> 
  sf::st_as_sf(coords = c("lon", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
  sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

#ar1, stipm, trailing arith mean, official preseason forecast
pal_lag_mod <- c("purple", "gold", "cyan", "pink") 

pdrw <- readRDS("O:/code/coho/forecast_wb/pdrw_2020.rds")
```

# Data

The analysis relies on per-stock escapement and harvest estimates compiled in post-season coho FRAM runs, as well as CWT-based marine survival estimates derived from RMIS, and smolt trap outmigrant estimates compiled by tribal comanagers and WDFW.

The first 4 subsections document the development of the complete dataset for the Oct 2021 review. The 2022 updates section captures subsequent extensions, particularly incorporating the 2020 return information.

## FRAM postseason estimates of spawning escapement and harvest mortality

Coho FRAM tracks age 3 fish across 5 time steps corresponding to a calendar year. It includes unmarked and marked units of both natural and hatchery stocks. 

Data in the best available database (maintained by PSC CoTC) extend to 1986, but note that values prior to 1998 are of unknown origin.

```{r read_and_export_fram_tables, eval=FALSE}
#the relevant data are queried and re-exported as csv
mdb <- "O:/code/coho_fram_validation/PSC_CoTC_PostSeason_CohoFRAMDB_thru2019_021021.mdb"
#appears to be a mysterious RunID 3 in the Escapement and Mortality tables but not RunID table...
#aggregate mortality across timesteps for all sources to "estimated total harvest related impacts"
fram <- full_join(
  read_coho_escapement(mdb, stocks = pop_meta$FRAM_StockID) |> 
    filter(!is.na(RunYear))
  ,
  read_coho_mort(mdb, stocks = pop_meta$FRAM_StockID) |>
    filter(!is.na(RunYear)) |> 
    mutate(mort = LandedCatch + NonRetention + Shaker + DropOff + MSFLandedCatch + MSFNonRetention + MSFShaker + MSFDropOff) |> 
    group_by(RunYear, StockID, StockLongName) |> 
    summarise(mort = sum(mort), .groups = "drop")
  ,
  by = c("RunYear", "StockID", "StockLongName")
  ) |> 
  select(year = RunYear, StockID, StockLongName, spwn = escp, hvst = mort)

#fram |> filter(StockID == 161) |> print(n = 100)

write_csv(fram, "data/postseason_fram_spwn_hvst_ests.csv")

```

## Smolt outmigrant estimates

Smolt trap estimates are included from a dataset maintained by Marisa Litz (WDFW).

```{r request_to_ML, eval=FALSE}
#tbl_coho |> filter(!is.na(`Smolt Abundance`), year >= 1986) |> count(pop)

smolt_used <- stan_data_filter(coho_data_tbl, data_year_min = 1986, data_year_max = 2021, lag_spwn = 1, lag_hvst = 1) |> 
  getElement("smolt") 
#writexl::write_xlsx(smolt_used, "O:/code/coho/forecast_wb/smolt_outmig_to_update.xlsx")

smolt_used |> group_by(pop) |> summarise(ymin = min(year), ymax = max(year))

#original script drops numerous smolt outmigrant series that apparently could not be well reconciled to FRAM units
smolt_full_orig <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> 
  mutate(
    pop = case_when(
      !is.na(`SaSI Population`) ~ `SaSI Population`,
      is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
      is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
    )
  ) |> 
  select(1:8, pop) |> 
  #count(Smolt_Abundance_Matches_FRAM)
  #count(Smolt_Abunce_Matches_SASI_but_not_FRAM)
  filter(!is.na(`Smolt Abundance`), `Calendar Year` >= 1986)
#see "drop_pop"
smolt_used |> count(pop, smolt_ocn_surv_pop) #9 units
smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) |> print(n = 50) #23 units

left_join(
  smolt_used |> count(pop, smolt_ocn_surv_pop)
  ,
  smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) #|> print(n = 50)
  , by = c("pop" = "Managment Unit (FRAM)")
  ) |> 
  filter(!(pop.y %in% drop_pop)) |> 
  select(-contains("."))

```

Updated data are read, wrangled and re-exported in format congruent with FRAM units.

```{r smolt_update_checks_export, eval=FALSE}

smolt <- readxl::read_excel("O:/code/coho/forecast_wb/Smolt Time Series.xlsx", na = "NA") |> 
  select(
    year = OEY,
    Chehalis_149 = Chehalis,
    Deschutes_63 = Deschutes,
    Dungeness_107 = Dungeness,
    Green_97 = Green,
    Nisqually_69 = Nisqually,
    Nooksack_1 = Nooksack,
    Queets_139 = Queets.Clear,
    Skagit_17 = Skagit,
    Snohomish_35 = Snohomish) |> 
  filter(year >= 1986) |> 
  pivot_longer(cols = -year, names_to = "smoltpop_StockID", values_to = "smolt_update") |> 
  separate(smoltpop_StockID, into = c("smoltpop","StockID"), sep = "_") |> 
  mutate(StockID = as.numeric(StockID)) |> 
  filter(!is.na(smolt_update)) #excel wide format has complete cases, so reduce to stock-years with data


full_join(
  smolt_used |> select(pop_id, pop, StockID, year, smolt_orig = smolt)
  ,
  smolt
  ,
  by = c("StockID", "year")
  ) |> #214
  mutate(
    smolt = if_else(is.na(smolt_update), smolt_orig, smolt_update)
  ) |> 
  #filter(is.na(smolt_orig)) #16 updated values for 2018-2020
  #filter(round(smolt_orig) != round(smolt_update)) #1991 Chehalis does not match
  #filter(is.na(smolt_update)) #prior year vals not present in update, 3 Chehalis and 1 Nooksack
  select(StockID, year, smolt) |> 
  write_csv("data/smolt_outmigrants.csv")

```

## Marine survival CWT releases and recoveries

Estimates of marine survival are included via records of coded wire tagged fish releases and recoveries. This dataset was updated by Neala Kendall and Ty Garber, following fix of RMIS release count error for tagged wild units. (Note the original dataset was unaffected because 1) error not present in hatchery units 2) previous wild unit values drawn from underlying pre-RMIS data.)

```{r cwt_rec_rel_update_checks_export, eval=FALSE}
# #strings match and all 15 units used in analysis are present
# #reversing order of terms shows the 3 unused dataset units: Baker H, Minter Crk H and Satsop H
# setdiff(
#   filter(pop_meta, !is.na(smolt_ocn_surv_pop)) |> distinct(smolt_ocn_surv_pop),
#   readxl::read_excel("O:/code/coho/forecast_wb/coho SARs_to extend.xlsx",
#     sheet = "CWT_FRAM_Matches_complete202108") |>
#     distinct(smolt_ocn_surv_pop = `Smolt Ocean Survival Population`)
# )

cwt <- readxl::read_excel(
  "O:/code/coho/forecast_wb/coho SARs_to extend.xlsx", 
  sheet = "CWT_FRAM_Matches_complete202108"
  ) |> 
  select(
    smolt_ocn_surv_pop = `Smolt Ocean Survival Population`,
    pop = `Managment Unit (FRAM)`,
    year = `Calendar Year`, est_n_rec = Fishery_Plus_Escapement, est_n_rel = Release_No
  ) |> 
  left_join(
    pop_meta |> mutate(pop_id = as.numeric(factor(pop))),
    by = c("pop", "smolt_ocn_surv_pop")
    ) |> 
  #filter(is.na(pop_id)) |> print(n = 100)
  #filter(str_detect(pop, "Area 13")) |> print(n = 100)
  filter(!is.na(pop_id)) |> 
  select(StockID, pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)

# #compare against prior
# tbl_coho |> 
#   filter(pop_id == 36, between(year, 1986, 2020)) |> 
#   select(pop_id, pop, `Smolt Ocean Survival Population`, year, Fishery_Plus_Escapement, Release_No) |> print(n = 50)

# full_join(
#   coho_data_tbl |> select(pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)
#   ,
#   cwt
#   ,
#   by = c("pop_id", "pop", "smolt_ocn_surv_pop", "year"), suffix = c("_orig", "")
#   ) |>
#   #filter(pop_id == 34, between(year, 1986, 2020)) |> print(n = 50)
#   mutate(
#     d_rec = est_n_rec - est_n_rec_orig,
#     d_rel = est_n_rel - est_n_rel_orig
#   ) |>
#   filter(
#     !is.na(d_rec), !is.na(d_rel),
#     (d_rec != 0 | d_rel != 0)) |>
#   select(pop_id:year, contains("_rel"), contains("_rec")) |> #print(n = 200)
#   writexl::write_xlsx("O:/code/coho/forecast_wb/coho_SARs_to_extend_diffs.xlsx")

cwt |> 
  filter(year >= 1986) |>
  select(StockID, year, est_n_rec, est_n_rel) |> 
  write_csv("data/rmis_cwts.csv")

```

## Combined into full dataset

This chunk performs the joins to generate the full dataset used in the current analysis.

```{r updating_full_dataset, eval=FALSE}
#Note the original negative escapement in A12A Wild in 2005 is due to an error in the FRAM database
#so still needs correcting: spwn is coerced to 0 here, but harvest is unaffected b/c drawn directly from Mortality
#note also Grays Hbr Misc Wild missing 2000 & 2001 in FRAM mdb and Area 13A Miscellaneous Wild missing 1996
#both fixed with simple insertion of NAs

#here the meta identifiers are (expanding left) joined to the FRAM data
#after these have been full joined to full joined CWT and smolt data

coho_data_tbl <- pop_meta |>
  full_join(
    full_join( #FRAM + (CWT + smolt)
      read_csv(fp$data_fram) |>
        select(-StockLongName) |>
        bind_rows(tibble(year = c(1996, 2000:2001), StockID = c(81, 157, 157))) |> 
        mutate(
          spwn = if_else(spwn < 0, 0.01, spwn),
          rtrn = spwn + hvst
        ) |>  
        arrange(StockID, year) #1156, 1986:2019
      ,
      full_join(
        read_csv(fp$data_cwt), #396
        read_csv(fp$data_smolt), #214 
        by = c("StockID", "year")
        )
      ,
      by = c("StockID", "year")
      )
    ,
    by = c("StockID"))

write_csv(coho_data_tbl, "data/fram_cwt_smolt_fulljoin.csv")

# coho_data_tbl |> count(StockID, pop_id, pop) |> print(n = 100)
# 
# #WB has FRAM through 2019, hatchery-based rel/rec through 2018, no smolt
# coho_data_tbl |> filter(StockID == 161) |> print(n = 50)
# 
# #Chehalis has the Bingham Crk numbers
# coho_data_tbl |> filter(StockID == 149) |> print(n = 50)
# 
# #Skagit is an example of "max data": FRAM to 2019, CWT to 2018, smolt to 2020
# coho_data_tbl |> filter(StockID == 17) |> print(n = 50)

```

## 2022 updates

Read 2020 escapement and harvest estimates from CoTC post-season run.

```{r fram_tables_2020, eval=FALSE}
#running the final run, but before transfer into the "clean" multiyear database
mdb <- "C:/Users/auerbdaa/Downloads/PSC_CoTC_PostSeason_CohoFRAMDB_thru2020working_021622_rv.mdb"
#aggregate mortality across timesteps for all sources to "estimated total harvest related impacts"
fram_2020 <- full_join(
  framr::read_coho_escp(mdb, stocks = sf_coord$StockID, runs = 73) |> 
    filter(!is.na(RunYear))
  ,
  framr::read_coho_mort(mdb, stocks = sf_coord$StockID, runs = 73) |>
    filter(!is.na(RunYear)) |> 
    group_by(RunYear, StockID, StockLongName) |> 
    summarise(hvst = sum(mort), .groups = "drop")
  ,
  by = c("RunYear", "StockID", "StockLongName")
  ) |> 
  mutate(
    year = as.numeric(RunYear),
    spwn = escp, escp = NULL,
    rtrn = spwn + hvst) |> 
  select(year, StockID, StockLongName, spwn, hvst, rtrn)

#fram |> filter(StockID == 161) |> print(n = 100)

```

```{r add_bkfram_2020, eval=FALSE}
#integrate CoTC 2020 postseason run results
#pre-2020 unchanged, but would prefer to rebuild from scratch
#if 2010-2019 can get re-run summer 2022...
#first left_join is simply adding metadata fields to avoid NAs for 2020 
coho_data_tbl_2020 <- rows_upsert(
  coho_data_tbl
  ,
  left_join(
    coho_data_tbl |> 
      distinct(pop_id, pop, StockID, StockLongName, lon, lat, hab_km, smolt_ocn_surv_pop, hat) |> 
      mutate(year = 2020)
    ,
    fram_2020
    # readxl::read_excel(
    #   "T:/DFW-Salmon Mgmt Modeling Team - General/Escapement files/Coho/fram_coho_escapement_2022.xlsx", 
    #   range = "BKFRAM!A4:Y168", col_types = c("numeric", "text", rep("numeric", length(1998:2020)))) |> 
    #   select(StockID = `Row Labels`, spwn = `2020`) |> 
    #   mutate(year = 2020)
    ,
    by = c("StockID", "StockLongName", "year")
  )
  ,
  by = c("StockID", "StockLongName", "year")
)

# coho_data_tbl |> filter(year >= 2020) |> print(n=50)
# coho_data_tbl_2020 |> filter(year >= 2020) |> print(n=50)

```

Include additional smolt migrant estimates.

```{r add_new_smolt, eval=FALSE}
##Jan 20 2022 email from Dr. Litz: "O:/code/coho/forecast_wb/Smolt Time Series Data.csv"
## initiated new sheet 
# coho_data_tbl |>
#   filter(!is.na(smolt)) |>
#   select(year, StockID, StockLongName, smolt) |>
#   arrange(year, StockID) |>
#   write.csv("clipboard", row.names = F)

#manually copied a few relevant 2021 values from "O:/code/coho/forecast_wb/Smolt Time Series Data.csv"
#updated Nooksack 2018-2020
#added Chehalis (backcalc'd) and Queets+Clearwater from email text
#no 2020 for Sno, Deschutes, Chehalis

coho_data_tbl_2020 <- rows_upsert(
  coho_data_tbl_2020
  ,
  #here have prior years, so append id cols, reversing pattern above
  left_join(
    readxl::read_excel("O:/code/coho/forecast_wb/Smolt Time Series.xlsx", sheet = "stipm_2022") |> 
      select(year, StockID, smolt) |> 
      filter(year > 2017)
    ,
    coho_data_tbl |> 
      distinct(pop_id, pop, StockID, StockLongName, lon, lat, hab_km, smolt_ocn_surv_pop, hat)
    ,
    by = c("StockID"))
  ,
  by = c("StockID", "year")) 

# coho_data_tbl_2020 |> filter(year >= 2020) |> print(n=50)

```

Add a 2021 preliminary escapement and harvest estimate for Willapa Bay only.

```{r add_2021_prelim_WB_and_export, eval=FALSE}
# filter(coho_data_tbl_2020, StockID==161, between(year, 2016, 2020)) |> 
#   ggplot(aes(spwn, hvst)) + geom_label(aes(label = year))
# #fit a basic log linear for 2016 forward
# #then predict from 23558 NOS+NOB prelim estimate
# #from 2022 WB4 Coho Forecast Model Draft 02.03.2022, sheet Spawning & Hatchery Escapements!CO33
# predict(
#   lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl_2020, StockID==161, between(year, 2016, 2020))),
#   filter(coho_data_tbl_2020, StockID==161, year==2020) |> 
#     mutate(year = 2021, spwn = 23558, hvst = NA, rtrn = NA)
# ) |> exp() #14148

coho_data_tbl_2020 <- coho_data_tbl_2020 |> 
  bind_rows(
    filter(coho_data_tbl_2020, StockID==161, year==2020) |> 
      mutate(year = 2021, spwn = 22558, hvst = 14148, rtrn = spwn + hvst)
  )

#preserved timestamped "data/fram_cwt_smolt_fulljoin_ssc_Oct2021.csv"
write_csv(coho_data_tbl_2020, "data/fram_cwt_smolt_fulljoin.csv")

```

```{r NOT_RUN_FINAL_add_pseudohvst, eval=FALSE}
# # coho_data_tbl |> filter(StockID==161, year>1997) |> ggplot(aes(year, hvst)) + geom_col()
# # filter(coho_data_tbl, StockID==161, year>1997) |> ggplot(aes(year, hvst/spwn)) + geom_col()
# # filter(coho_data_tbl, StockID==161, year>1997) |> ggplot(aes(year, hvst/(hvst+spwn))) + geom_col()
# # 
# # coho_data_tbl |> filter(StockID==161, year>1997) |> ggplot(aes(spwn, hvst, color = year)) + geom_point() + scale_x_continuous(limits = c(0,100000)) + scale_y_continuous(limits = c(0,100000))
# # 
# # coho_data_tbl |> filter(StockID==161, year>1997) |> ggplot(aes(log(spwn), log(hvst), color = year, label = year)) + geom_point() + geom_text() + geom_smooth(method = "lm")# + facet_wrap(~year>2015)
# # 
# # summary(lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl, StockID==161, year>1997)))
# # #tightening to recent years in lower return regime (also boosts r2) 
# # summary(lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl, StockID==161, year>=2016)))
# # predict(
# #   lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl, StockID==161, between(year, 2016, 2019))),
# #   filter(coho_data_tbl_2020, StockID==161, year==2020)
# #   ) |> exp() #6270
# 
# # #add just WB
# coho_data_tbl_2020 <- rows_upsert(
#   coho_data_tbl_2020
#   ,
#   filter(coho_data_tbl_2020, StockID==161, year==2020) |>
#     mutate(hvst = 6270, rtrn = spwn + hvst)
#   ,
#   by = c("StockID", "year"))
# 
# # ## OR add stocks with decent correlations and r2 in recent years
# # coho_data_tbl_2020 <- rows_upsert(
# #   coho_data_tbl_2020
# #   ,
# #   coho_data_tbl |> 
# #     filter(between(year, 2016, 2019)) |> 
# #     group_by(StockID, StockLongName) |> nest() |> #very similar to nest_by
# #     summarise(
# #       corr = map_dbl(data, ~cor(log(.x$hvst), log(.x$spwn))),
# #       fit = map(data, ~lm(log(hvst) ~ log(spwn), data = .x)),
# #       .groups = "keep"
# #     ) |> 
# #     left_join(
# #       filter(coho_data_tbl_2020, year == 2020) |> select(StockID, StockLongName, spwn)
# #       , by = c("StockID", "StockLongName")
# #     ) |> 
# #     nest(newdata = spwn) |> 
# #     mutate(
# #       r2 = map_dbl(fit, ~summary(.x) |> pluck("r.squared")),
# #       hvst = map2_dbl(fit, newdata, ~predict.lm(.x, newdata = .y) |> exp())
# #     ) |>
# #     unnest(newdata) |> ungroup() |> 
# #     arrange(desc(r2, corr)) |> #print(n = 50)
# #     filter(corr >= 0.8, r2 > 0.8, !is.na(hvst)) |> 
# #     mutate(rtrn = spwn + hvst, year = 2020) |> 
# #     select(StockID, StockLongName, year, spwn, hvst, rtrn)  
# #   ,
# #   by = c("StockID", "year"))

```

# Model fitting

The one-ahead forecast skill evaluation that was demonstrated in the Oct 2021 review is expected to be performed for the 2023 effort, but it was not re-run during the 2022 exercise due to the limited dataset changes and the minor performance differences between the 2 current model configurations.

## `stan` functions

Forecast skill evaluation requires re-fitting models to progressive subsets of the data filtered by year. Rather than a "sliding window" (e.g., the prior 10 years stepped forward each year), the dataset is "stretched" from a fixed starting year. Here, a helper function trims years from the complete dataset according to arguments controlling the data-type-specific lag from a data `data_year_max` corresponding to the year before the desired forecast year (i.e., a `data_year_max` of 2021 designates the most recent possible data for the 2022 forecast). 

```{r stan_data_filter}
#trim years then declare separate intermediaries
#"data_year_max" is "last/max year of available data", so desired year_pred-1
#lags allow for flexible OAT configurations with additional trimming
#but can be zeroed to allow all available data
#for a 2022 forecast in adult_pred vectors from stan
#data_year_max is 2021, s.t. firmly available postseason FRAM is 2019 
stan_data_filter <- function(
  full, #complete dataset
  data_year_min, #starting anchor year
  #in STIPM, adult_pred[data_year_max+1] is a total return from smolt[data_year_max]*surv[data_year_max], with adult_est[data_year_max] as the spawning escapement
  #in AR1, adult_est and adult_pred are total return, and adult_pred[year_pred] is same as last year of adult_est b/c passing n_year+1 relative to STIPM
  data_year_max,
  lag_smolt = 0, #n-extra-years trimmed from smolt abundances 
  lag_MS = 1, #n-extra-years trimmed from release and recovery data
  lag_spwn = 2, #n-extra-years trimmed from FRAM escapement
  lag_hvst = 2 #n-extra-years trimmed from FRAM harvest
  ){
  
  full_ymin_ymax = filter(full, between(year, data_year_min, data_year_max)) |> 
    arrange(pop_id, year) |>  #ensure everything sorted to prevent slice point mishaps
    mutate(yr = year - min(year) + 1)  #reindexing for stan
  
  stan_data = list(
    y_min_max = data_year_min:data_year_max,
    smolt = filter(full_ymin_ymax, !is.na(smolt), year <= data_year_max - lag_smolt),
    MS = filter(full_ymin_ymax, !is.na(est_n_rec), year <= data_year_max - lag_MS),
    spwn = filter(full_ymin_ymax, !is.na(spwn), year <= data_year_max - lag_spwn),
    hvst = filter(full_ymin_ymax, !is.na(hvst), year <= data_year_max - lag_hvst),
    rtrn = filter(full_ymin_ymax, !is.na(rtrn), year <= data_year_max - max(c(lag_spwn, lag_hvst)))
  )
  
  return(stan_data)
}

# #tests, random start year
# stan_data_filter(coho_data_tbl, 2004, 2006) |> map(tail) #predicting 2007, with defaults only get FRAM through 2004
# stan_data_filter(coho_data_tbl, 2004, 2007) |> map(tail) #predicting 2008, defaults give FRAM 2005

```

Next, wrappers to the `rstan::stan` function facilitate repeated fitting with consistent control parameters and input data lists.

```{r ar1_functions}
stan_ar1 <- function(stan_data, n_iter = 200, n_chain = 2){
  #note following orig naming convention where "tot" refers to "total return" = spwn + hvst = rtrn, as estimated from FRAM
  stan_fit <- stan(
    file = 'LD_coho_forecast_AR_ind_2_v4.stan',
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.999, max_treedepth = 12),
    data = list(
      n_year = length(stan_data$y_min_max) + 1, #sets year of adult_pred and year-dim of adult_est 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      n_pop_tot = length(unique(stan_data$rtrn$pop_id)), #Number of populations with return data
      pop_tot = unique(stan_data$rtrn$pop_id), #Which populations possess return data
      n_tot = length(stan_data$rtrn$rtrn), #Length of the return data vectors
      tot_dat = stan_data$rtrn$rtrn,  #Vectors of all return data across all populations
      tot_true = stan_data$rtrn$yr, #Vectors of the indices identifying which years are those with non-NA data for the return data
      #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
      slice_tot_start = pmatch(unique(stan_data$rtrn$pop_id), stan_data$rtrn$pop_id), #stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_tot_end = c(tail(pmatch(unique(stan_data$rtrn$pop_id), stan_data$rtrn$pop_id)-1, -1), length(stan_data$rtrn$pop_id)) #stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  
  return(stan_fit)
}

```

```{r stipm_functions}
stan_stipm <- function(stan_data, n_iter = 200, n_chain = 2){
  #note n_year does not include "+1" of AR1 since adult_pred is calc'd separately as
  # smolt[n_year]*surv[n_year], thereby giving adult run size (spwn+hvst) in n_year+1
  #stan_data$y_min_max accordingly is first:last_year_of_data
  stan_fit <- stan(
    file = 'LD_coho_forecast_6_2_4.stan', 
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.99, max_treedepth = 10.25),
    data = list(
      n_year = length(stan_data$y_min_max), 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      u = matrix(1, nrow = 1, ncol = length(unique(stan_data$spwn$pop_id))),
      dist = units::drop_units(sf::st_distance(sf_coord)/10000), #values are identical
      
      pop_smolt = unique(stan_data$smolt$pop_id), #pop_ids with smolt data 
      n_pop_smolt = length(unique(stan_data$smolt$pop_id)),
      smolt_true = stan_data$smolt$yr,
      smolt_dat = stan_data$smolt$smolt,
      n_smolt = nrow(stan_data$smolt),
      
      pop_esc = unique(stan_data$spwn$pop_id),  # pop_ids with escapement data
      n_pop_esc = length(unique(stan_data$spwn$pop_id)),
      esc_true = stan_data$spwn$yr,
      esc_dat = stan_data$spwn$spwn,
      n_esc = nrow(stan_data$spwn),
      
      pop_catch = unique(stan_data$hvst$pop_id), #pop_ids with harvest data
      n_pop_catch = length(unique(stan_data$hvst$pop_id)),
      harvest_true = stan_data$hvst$yr,
      harvest_dat = stan_data$hvst$hvst,
      n_harvest = nrow(stan_data$hvst),
      
      pop_MS = unique(stan_data$MS$pop_id), #pop_ids with marine survival data
      n_pop_MS = length(unique(stan_data$MS$pop_id)),
      MS_true = stan_data$MS$yr, #orig uses Fishery_Plus_Escapement rather than Release_No to filter...
      MS_dat_x = stan_data$MS$est_n_rec |> round() |> as.integer(), #stan expects integer; previously labeled Fishery_Plus_Escapement
      MS_dat_N = stan_data$MS$est_n_rel |> round() |> as.integer(), #previously labeled Release_No
      n_MS = nrow(stan_data$MS),
      
      stream_dist = stan_data$spwn |> distinct(pop, hab_km) |> pluck("hab_km"),
      
      sigma_esc = 0.2,
      
      n_hatchery = filter(stan_data$MS, hat == 1) |> distinct(pop_id) |> nrow(),
      hatchery = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat > 0, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),
      wild = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat < 1, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),

      slice_smolt_start = pmatch(unique(stan_data$smolt$pop_id), stan_data$smolt$pop_id), #stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_smolt_end = c(tail(pmatch(unique(stan_data$smolt$pop_id), stan_data$smolt$pop_id)-1, -1), length(stan_data$smolt$pop_id)), #stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_esc_start = pmatch(unique(stan_data$spwn$pop_id), stan_data$spwn$pop_id), #stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_esc_end = c(tail(pmatch(unique(stan_data$spwn$pop_id), stan_data$spwn$pop_id)-1, -1), length(stan_data$spwn$pop_id)), #stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_harvest_start = pmatch(unique(stan_data$hvst$pop_id), stan_data$hvst$pop_id), #stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_harvest_end = c(tail(pmatch(unique(stan_data$hvst$pop_id), stan_data$hvst$pop_id)-1, -1), length(stan_data$hvst$pop_id)), #stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_MS_start = pmatch(unique(stan_data$MS$pop_id), stan_data$MS$pop_id), #stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_MS_end = c(tail(pmatch(unique(stan_data$MS$pop_id), stan_data$MS$pop_id)-1, -1), length(stan_data$MS$pop_id)) #stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  return(stan_fit)
}
```

## One-ahead *post-hoc* forecast

This section not run for 2022, but left inline in anticipation of being revisited for 2023. The alternative data lags examined for the Oct 2021 review will be reduced to a single configuration, reflecting the actual experience of application in 2022.

The one-at-a-time model fitting proceeds by iterating through a vector of "max data years", predicting the following year by first filtering down a subset of the estimation data, then calling the stan models, and then extracting diagnostic and posterior results.

```{r oat, eval=FALSE}
#intermediary single year output is saved out within map() in case of loop disruptions
#a dataset data_year_max of 2014, predicting 2015, with default lags of 2 gives FRAM/spwn through 2012
#but with "compiling" lags of 1 gives FRAM/spwn through 2013

oat_wrap <- function(iter, #3K 
                     chain, #4
                     oat_data_year_min, #1998,
                     oat_years_max, #2008:2018, #max data year, predicting 2009:2019
                     lag_spwn, lag_hvst, lag_MS, lag_smolt #deducted from data year
                     ){
  #build a list named by max data year 
  oat <- set_names(oat_years_max) |> 
    map(function(x) {
      print(paste("max data year", x, "predicting", x+1))
      stan_data_list <- stan_data_filter(coho_data_tbl, data_year_min = oat_data_year_min, data_year_max = x, lag_spwn = lag_spwn, lag_hvst = lag_hvst, lag_MS = lag_MS, lag_smolt = lag_smolt) 
      print(stan_data_list$y_min_max)
      print(c("smolt", range(stan_data_list$smolt$year)))
      print(c("MS", range(stan_data_list$MS$year)))
      print(c("spwn", range(stan_data_list$spwn$year)))
      print(c("hvst", range(stan_data_list$hvst$year)))
      print(c("rtrn", range(stan_data_list$rtrn$year)))

      print(paste("AR1 start", Sys.time()))

      fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall AR1 stan data arg n_year+1 relative to STIPM gives adult_pred in desired year
      fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "ar1",
          n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      print(paste("STIPM start", Sys.time()))

      fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
      fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "stipm",
          n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      pred_out <- bind_rows(fit_ar1_pred, fit_stipm_pred)
      saveRDS(pred_out, paste0("oat_fits/oat_pred_",x+1,"_lags_spwn",lag_spwn,"_hvst",lag_hvst,"_MS",lag_MS,"_smolt",lag_smolt,".rds"))
      return(pred_out) #list element for max-data-year-x 
    })
  return(oat)
}

# #REDUCE TO A SINGLE CONFIGURATION REFLECTING 2022 APPLICATION
# #predicting 2009:2019; renaming objects with lags relative to prediction year (rather than data year, as in args)
# oat_lag3 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 2, lag_hvst = 2, lag_MS = 1, lag_smolt = 0)
# oat_lag2 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 1, lag_hvst = 1, lag_MS = 1, lag_smolt = 0)
# oat_lag1 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 0, lag_hvst = 0, lag_MS = 1, lag_smolt = 0)

```

# Results

As above, this section not run for 2022, but left inline in anticipation of being revisited for 2023, at which time the comparison of alternative data lags can be removed.

Saved output from the one-ahead runs is read back into memory and joined with observations to allow examination of performance skill.

```{r read_bind_oat_fits, eval=FALSE}
# #tibbles of annual predictions for AR1 & STIPM with a given lag
# bind_rows(
#   list.files("oat_fits", pattern = "lags_spwn0_hvst0", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l1_", mod))
#   ,
#   list.files("oat_fits", pattern = "lags_spwn1_hvst1", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l2_", mod))
#   ,
#   list.files("oat_fits", pattern = "lags_spwn2_hvst2", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l3_", mod))
# ) |>
#   select(lag_mod, year, StockID, pop_id, pop, n_diverg, n_eff, Rhat, `2.5%`:`97.5%`) |>
#   saveRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

```

For comparison, generate deterministic trailing means and read official preseason forecast compilation.

```{r trailing_means_and_fcst, eval=FALSE}
#create a temp col of the best case values that would be available in a given forecast year (i.e., up to year - 1)
#then create rolling mean over prior values
#in addition, join the compilation of submitted forecasts from Marlene Bellman
rtrn_trail_mean_fcst <- coho_data_tbl |> 
  select(StockID, pop_id, pop, year, rtrn) |>
  group_by(StockID, pop_id, pop) |> 
  mutate(
    lag3_rtrn = c(NA, NA, NA, head(rtrn, -3)),
    l3_trail_mean = slider::slide_dbl(lag3_rtrn, ~mean(., na.rm=T), .before = 2),
    lag2_rtrn = c(NA, NA, head(rtrn, -2)),
    l2_trail_mean = slider::slide_dbl(lag2_rtrn, ~mean(., na.rm=T), .before = 2),
    lag1_rtrn = c(NA, head(rtrn, -1)),
    l1_trail_mean = slider::slide_dbl(lag1_rtrn, ~mean(., na.rm=T), .before = 2)
  ) |> 
  ungroup() |> 
  left_join(
    #forecast of record compilation; not public
    read_csv("O:/code/coho/forecast_wb/ForecastCompilation_SourceDataPre_MB080520.csv") |> 
      select(StockID, year = YEAR, pre_fcst = RunSize)
    , by = c("year", "StockID")
  ) |> 
  filter(between(year, 2009, 2019)) |> select(-starts_with("lag")) |>  
  pivot_longer(cols = c(contains("trail"), pre_fcst), names_to = "lag_mod", values_to = "50%") 

```

These are bound together and performance measures are calculated.

```{r read_oat_and_oat_obs, eval=FALSE}
oat <- readRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

(
oat_obs <- left_join(
  bind_rows(
    oat |> select(lag_mod, year, StockID, pop_id, pop, `2.5%`:`97.5%`),
    rtrn_trail_mean_fcst |> select(-rtrn) #will add back in next join
    ),
  coho_data_tbl |> 
    select(year, StockID, rtrn) |> 
    group_by(StockID) |>  #adding MASE denominator
    mutate(scale_err = mean(abs(rtrn - c(NA, head(rtrn, -1))), na.rm = T)) |> 
    ungroup(),
  by = c("year", "StockID")
  ) |>
  mutate(
    in_50 = (rtrn >= `25%`) & (rtrn <= `75%`),
    in_95 = (rtrn >= `2.5%`) & (rtrn <= `97.5%`),
    #convention: actual - forecast
    #possibly counterintuitive, but
    #negative is overforecast ("fewer returned than predicted"),
    #positive is underforecast ("more returned than predicted")  
    err = rtrn - `50%`,
    err_log = log(rtrn) - log(`50%`),
    err_abs_pct = abs(err / rtrn),
    lar = log(`50%`/rtrn), #log accuracy ratio, used for median symmetric accuracy as described in Morley et al 2018
    ase = abs(err) / scale_err #absolute scaled error for MASE
  ) |> 
  arrange(year, StockID, lag_mod)
)


#oat |> count(lag_mod) ; oat_obs |> count(lag_mod) 
#oat_obs |> select(lag_mod, err:ase) |> summary()
```

## Diagnostics

```{r oat_rhat_neff_n_diverg, eval=FALSE}
# #all years, all pops
# oat |> 
#   group_by(lag_mod) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop")

# #by year, all pops
# oat |> 
#   group_by(lag_mod, year) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop") |> 
#   #  print(n = 100)
#   gt::gt(rowname_col = "lag_mod") |>
#   gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |>
#   gt::fmt_number(contains("Rhat"), decimals = 2) |>
#   gt::fmt_number(contains("n_eff"), decimals = 0)

#Willapa, all years, similar to "all pops"
oat |> 
  filter(StockID == 161) |> 
  # arrange(lag_mod, year) |>
  # #filter(str_detect(lag_mod, "stipm")) |> 
  # print(n = 100)
  group_by(lag_mod) |> 
  summarise(
    n_diverg = max(n_diverg),
    Rhat_med = median(Rhat), Rhat_max = max(Rhat),
    n_eff_med = median(n_eff), n_eff_min = min(n_eff),
    .groups = "drop") |> 
  gt::gt(rowname_col = "lag_mod") |> 
#  gt::cols_hide(c("pop_id", "pop")) |> 
  gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "2009-19, Willapa Bay Natural") |> 
  gt::fmt_number(contains("Rhat"), decimals = 2) |> 
  gt::fmt_number(contains("n_eff"), decimals = 0)

# #n_eff boxes
# oat |> 
#   filter(StockID == 161, str_detect(lag_mod, "stipm")) |> 
#   ggplot(aes(lag_mod, n_eff)) + geom_boxplot() + geom_jitter(width = 0.1)
# #n_eff col timeseries, arguably slightly better after 2012
# oat |> 
#   filter(StockID == 161, str_detect(lag_mod, "stipm")) |> 
#   ggplot(aes(factor(year), n_eff, fill = lag_mod)) + geom_col(position = "dodge") + scale_fill_grey()

```

## One-ahead Performance

### Lag3 paneled by year

Posterior medians and 95% CIs are shown for the AR1 (purple) and STIPM (gold) for each individual forecast year relative to the filtered data subset (darker lines) and the subsequent return observations. Also shown are the deterministic lagged trailing mean (cyan) and the submitted forecast of record (pink).

```{r oat_obs_pred_patchwork, fig.width=8, fig.height=10, eval=FALSE}
set_names(2008:2018) |> 
  map(function(x) {
    d <- coho_data_tbl |> filter(StockID == 161, between(year, 1998, x)) |> select(year, StockID, pop, rtrn)

    oat_obs |> 
      filter(StockID == 161, str_detect(lag_mod, "l3|pre_fcst"), year == x+1) |> 
      ggplot() +
      #all pre-forecast data
      geom_line(data = d, aes(x = year, y = rtrn), color = grey(0.8), size = 0.9) +
      geom_point(data = d, aes(x = year, y = rtrn), color = grey(0.8), size = 0.9) +
      #available data for L3
      geom_line(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), color = grey(0.4), size = 0.7) +
      geom_point(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), color = grey(0.4), size = 0.7) +
      #observed in forecast year
      geom_point(aes(x = year, y = rtrn), color = 1, shape = 17)  +
      #forecasts
      geom_pointrange(aes(x = year, y = `50%`, ymin = `2.5%`, ymax = `97.5%`, color = lag_mod), fatten = 1.02, position = position_dodge(width = 0.5), show.legend = F) +
      geom_point(aes(x = year, y = `50%`, color = lag_mod), size = 0.9, position = position_dodge(width = 0.5), show.legend = F) +
      geom_vline(xintercept = x-2, linetype = "dotted", size = 1) +
      scale_x_continuous(name = "", limits = c(1998, 2020), breaks = 1998:2020, labels = 1998:2020) +
#      scale_y_continuous(name = "Return", limits = c(0,350000), labels = scales::comma) + #limits drops point error when outside ymax
      scale_y_continuous(name = "Return", labels = scales::comma) + #limits drops point error when outside ymax
      scale_color_manual(name = "", values = pal_lag_mod) +
      theme(axis.text = element_text(size = 6), axis.title = element_text(size = 5))

}) |> 
  patchwork::wrap_plots(ncol = 1)

ggsave("O:/code/coho/forecast_wb/f_oat_l3_by_pred_year.png", width = 7, height = 9)
```

### Lag3 50th with ribbons

The panels of years in the prior plot are collapsed, with ribbons illustrating the 50th and 95% CI. 

```{r l3_50th_ribbons, fig.width=8, fig.height=8, eval=FALSE}
oat_ribbon <- function(stk = 161, lg_md = "l3"){
  d <- oat_obs |> filter(StockID == stk, str_detect(lag_mod, lg_md))
  g <- ggplot(d, aes(year)) + 
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_point(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn)) +
    geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`, color = lag_mod, fill = lag_mod), alpha = 0.2, linetype = 0) + 
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`, color = lag_mod, fill = lag_mod), alpha = 0.4, linetype = 0) + 
    geom_line(aes(y = `50%`, color = lag_mod), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", labels = scales::comma) +
    scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
    theme(legend.position = "top") +
    labs(title = d$pop[1])
  return(g)
}

g <- oat_ribbon(lg_md = "l3|pre_fcst")
g + facet_wrap(~lag_mod)

```

### Lag3 error time series

The raw prediction error (observed - forecast) is shown for the state space models alongside the deterministic mean and the recorded FRAM forecast.

```{r l3_error_series, fig.width=7, fig.height=7, eval=FALSE}
ggdata <- oat_obs |> 
  filter(StockID == 161, str_detect(lag_mod, "l3|pre_fcst")) |> 
  arrange(year, lag_mod) |> 
  mutate(year = as.character(year))

ggplot(ggdata, aes(year)) + 
  geom_col(aes(y = err, color = lag_mod, fill = lag_mod), position = position_dodge(), width = 0.7) +
  annotate("text", "2013", 8e4, label = "Underforecast:\n more returned than expected") +
  annotate("text", "2017", -5.5e4, label = "Overforecast:\n fewer returned than expected") +
  scale_x_discrete(name = "", guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(name = "Error: observed - forecast", labels = scales::comma) + #limits drops point error when outside ymax
  scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
  theme(legend.position = "top")

ggsave("O:/code/coho/forecast_wb/f_oat_l3_error.png", width = 7, height = 5)

```

### Overlaid posterior medians by lag

```{r oat_lag_overlay, fig.width=7, fig.height=7, eval=FALSE}
oat_lag_overlay <- function(stk = 161){
  d <- oat_obs |> filter(StockID == stk, !str_detect(lag_mod, "pre_")) |> 
    mutate(
      lag = paste("Lag", str_sub(lag_mod, 2,2)),
      mod = toupper(str_sub(lag_mod, 4, 40))
    )
 
  ggplot(d, aes(year)) + 
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_point(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_line(aes(y = `50%`, color = lag), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2), minor_breaks = 1998:2020) +
    scale_y_continuous(name = "Return", labels = scales::comma) + 
    scale_color_manual("", values = as.vector(wacolors::wa_pal("rainier", which = c("ground","winter_sky","paintbrush"))), aesthetics = c("color", "fill")) +
    facet_wrap(~mod, ncol = 1) + 
    theme(legend.position = "top") +
    labs(title = d$pop[1], color = "", fill = "")
  
}

oat_lag_overlay(161)

#ggsave("O:/code/coho/forecast_wb/f_oat_pred_lag_overlay.png", width = 8, height = 9)

oat_lag_overlay(131) + #Quill Fall
oat_lag_overlay(153) +  #Hump
oat_lag_overlay(149) + #Chehalis
oat_lag_overlay(157) + #GH Misc
oat_lag_overlay(63) + #Deschutes
oat_lag_overlay(69) + #Nisq
patchwork::plot_layout(ncol = 2)
```

### Summarized performance measures

```{r gt_perf_measures, eval=FALSE}
summary_start_year <- 2009

oat_obs |>
  filter(between(year, summary_start_year, 2019)) |> 
  arrange(year, lag_mod) |>
  group_by(StockID, pop, lag_mod) |> 
  summarise(
    msa = 100*(exp(median(abs(lar))) - 1),
    mase = 100*mean(ase),
    # mape = 100*mean(err_abs_pct),
    # rmse = sqrt(mean(err^2)),
    # # me = mean(err),
    # # mpe = 100*median(err_pct),
    # # nse_mod = 1 - ( sum(err_abs) / sum(abs(rtrn - mean(rtrn))) ),
    # # nse = 1 - ( sum(err^2) / sum((rtrn - mean(rtrn))^2) ),
    .groups = "drop") |> 
  mutate(
    lag = str_sub(lag_mod, 2,2),
    lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
    mod = toupper(str_sub(lag_mod, 4, 40)),
    mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) |> 
  filter(StockID == 161) |>
  gt::gt(rowname_col = "mod", groupname_col = "lag") |> 
  gt::cols_hide(c(StockID, pop, lag_mod)) |> 
  gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |> 
  # gt::fmt_percent(columns = c(msa, mase, mape), scale_values = F, decimals = 0) |> 
  # gt::fmt_number(columns = c(rmse), decimals = 0) |> 
  gt::data_color(
    columns = c(msa, mase), 
    colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
  ) |> 
  gt::tab_header(
    title = "One-ahead Performance Measures", 
    subtitle = paste(summary_start_year, "- 2019; AR1 and STIPM calculated from posterior median")) |> 
  gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
  gt::tab_source_note("Mean Abs. Scaled Error (MASE)") |> 
  # gt::tab_source_note("Mean Abs. Percent Error (MAPE)") |> 
  # gt::tab_source_note("Root Mean Square Error (RMSE)") |> 
  # #gt::tab_source_note("Mean Error (ME)")

gt::gtsave("O:/code/coho/forecast_wb/gt_oat_perf_measures_09_19.png", expand = 20)


perf_smry <- function(stk, summary_start_year = 2009){
  gt_data <- oat_obs |>
    filter(StockID == stk, between(year, summary_start_year, 2019)) |> 
    arrange(year, lag_mod) |>
    group_by(StockID, pop, lag_mod) |> 
    summarise(
      msa = 100*(exp(median(abs(lar))) - 1),
      mase = 100*mean(ase),
      .groups = "drop") |> 
    mutate(
      lag = str_sub(lag_mod, 2,2),
      lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
      mod = toupper(str_sub(lag_mod, 4, 40)),
      mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) 
  
   gt_data |> 
    gt::gt(rowname_col = "mod", groupname_col = "lag") |>
    gt::cols_hide(c(StockID, pop, lag_mod)) |>
    gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |>
    gt::data_color(
      columns = c(msa, mase),
      colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
    ) |>
    gt::tab_header(
      title = paste("One-ahead Performance:", gt_data[1,"pop"], gt_data[1,"StockID"]),
      subtitle = paste(summary_start_year, "- 2019; AR1 and STIPM calculated from posterior median")) |>
    gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
    gt::tab_source_note("Mean Abs. Scaled Error (MASE)")
}

perf_smry(161)

# coho_data_tbl |> distinct(StockID) |> pluck("StockID") |> 
#   walk(~perf_smry(.x) |> gt::gtsave(paste("O:/code/coho/forecast_wb/gt_oat_summary/gt_oat_perf_measures_stk",.x, ".png"), expand = 20))

```

# Full dataset

This chunk performs the actual fits, with the final set of commented lines left for convenience when running more iterations and chains on AWS.

```{r full_data_fit}
#use all data
(
  stan_data_list <- stan_data_filter(
    coho_data_tbl,
    #coho_data_tbl_2020,
    data_year_min = 1998, data_year_max = 2021, 
    lag_smolt = 0, lag_MS = 0, lag_spwn = 0, lag_hvst = 0)
)

# #can construct estimates beyond desired forecast year by "tricking" the wrapper that sets n_year
# #then retain fit objects and extract prediction as adult_est (for AR1) and adult_est+harvest_est (for STIPM)
# stan_data_list$y_min_max <- 1998:2021

# print(stan_data_list$y_min_max)
# print(c("smolt", range(stan_data_list$smolt$year)))
# print(c("MS", range(stan_data_list$MS$year)))
# print(c("spwn", range(stan_data_list$spwn$year)))
# print(c("hvst", range(stan_data_list$hvst$year)))
# print(c("rtrn", range(stan_data_list$rtrn$year)))
# 
# fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = 500, n_chain = 4)
# fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = 500, n_chain = 4)

# # #on AWS
# fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = 1000, n_chain = 4)
# saveRDS(fit_stipm, "fit_stipm_2020.rds")
# 
# fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = 1000, n_chain = 4)
# saveRDS(fit_ar1, "fit_ar1_2020.rds")

```

## Output objects

This chunk extracts adult estimates and is somewhat superseded by `pdrw` object below, but left in case. The first block extracts draws in a `tidybayes` flow while the second uses the `rstan` basic `summary()$summary` mode.

```{r read_full_data_fits, eval=FALSE}
# #these are huge, balloons memory
# #per year have rows = 4chains * post-warmup
# 
# # Oct2021: 
# "O:/code/coho/forecast_wb/ssc_oct_2021/full_data_fit_ar1.rds"
# "O:/code/coho/forecast_wb/ssc_oct_2021/full_data_fit_stipm.rds"

# fit_ar1 <- readRDS("O:/code/coho/forecast_wb/full_data_fit_ar1_2020.rds")
# fit_stipm <- readRDS("O:/code/coho/forecast_wb/full_data_fit_stipm_2020.rds")

fit_ar1_wb <- spread_draws(fit_ar1, adult_est[n_year,n_pop] | n_pop) |>
  ungroup() |> select(n_year, .iteration, rtrn = `34`) |>
  mutate(year = first(stan_data_list$y_min_max)-1+n_year, mod = "ar1")
 
# saveRDS(fit_ar1_wb, "fit_ar1_wb_adult_est.rds")
 
# #stipm requires constructing total return from harvest + spawning adults
fit_stipm_wb <- full_join(
  spread_draws(fit_stipm, adult_est[n_year,n_pop] | n_pop) |>
    ungroup() |> select(n_year, .chain, .iteration, .draw, spwn = `34`)
  ,
  spread_draws(fit_stipm, harvest_est[n_year,n_pop] | n_pop) |>
    ungroup() |> select(n_year, .chain, .iteration, .draw, hvst = `34`)
  ,
  by = c("n_year", ".chain", ".iteration", ".draw")) |>
  mutate(year = first(stan_data_list$y_min_max)-1+n_year, rtrn = spwn+hvst, mod = "stipm") |> 
  bind_rows(
    spread_draws(fit_stipm, adult_pred[n_pop] | n_pop) |> ungroup() |> 
      select(.chain, .iteration, .draw, rtrn = `34`) |> 
      mutate(year = last(stan_data_list$y_min_max)+1, mod = "stipm")
  )
 
# saveRDS(fit_stipm_wb, "fit_stipm_wb_adult_est.rds")

#rstan summary()$summary 
fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
    mod = "ar1",
    n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")
#recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

fit_ar1_pred |> 
  filter(between(StockID, 130, 170)) |> 
  print(n=40)
fit_stipm_pred |> 
  filter(between(StockID, 130, 170)) |> 
  print(n=40)

bind_rows(fit_ar1_pred, fit_stipm_pred) |> 
  filter(between(StockID, 130, 170)) |> 
  select(year, mod, StockID, StockLongName, pop_id, `2.5%`,`25%`,`50%`,`75%`,`97.5%`) |> 
  pivot_wider(names_from = mod, values_from = `2.5%`:`97.5%`) |> 
  select(year:pop_id, contains("50%"))

```

This chunk extracts the posterior distributions of per-year, per-stock return estimates for both the AR1 and STIPM model fits.

```{r pdrw, eval=FALSE}
pdrw <- bind_rows(
  #stipm
  #pre-fcst year estimate series
  full_join(
    spread_draws(fit_stipm, adult_est[n_year,n_pop] | n_pop) |> ungroup() |> 
      pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "spwn"),
    spread_draws(fit_stipm, harvest_est[n_year,n_pop] | n_pop) |> ungroup() |> 
      pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "hvst"),
    by = c("n_year", ".chain", ".iteration", ".draw", "pop_id")
  ) |>
    mutate(
      year = first(stan_data_list$y_min_max)-1+n_year, n_year = NULL,
      rtrn = spwn+hvst,
      mod = "stipm") |> 
    #add fcst year
    bind_rows(
      spread_draws(fit_stipm, adult_pred[n_pop] | n_pop) |> ungroup() |> 
        pivot_longer(cols = -c(.chain, .iteration, .draw), names_to = "pop_id", values_to = "rtrn") |> 
        mutate(year = last(stan_data_list$y_min_max)+1, mod = "stipm")
    )
  ,
  #ar1
  spread_draws(fit_ar1, adult_est[n_year,n_pop] | n_pop) |> ungroup() |>
    pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "rtrn") |> 
    mutate(
      year = first(stan_data_list$y_min_max)-1+n_year, n_year = NULL,
      mod = "ar1")  
  ) |> 
  mutate(pop_id = as.numeric(pop_id)) |> 
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

# #from AWS
# saveRDS(pdrw, "pdrw_2020.rds")
```

## Return plots

Similarly, this chunk is somewhat superseded by the `ggfcst_ts` below, but left for now.

```{r wb_only_ribbons, eval=FALSE}
# fit_ar1_wb <- readRDS( "fit_ar1_wb_adult_est.rds")
# fit_stipm_wb <- readRDS("fit_stipm_wb_adult_est.rds")

est_wb_rtrn <- bind_rows(
  fit_ar1_wb |> select(mod, year, rtrn), 
  fit_stipm_wb |> select(mod, year, rtrn)
  )

est_wb_rtrn_q <- est_wb_rtrn |> 
  group_by(mod, year) |> 
  summarise(across(rtrn, list(
    `2.5` = ~quantile(.x, 0.025),
    `25` = ~quantile(.x, 0.25),
    `50` = ~quantile(.x, 0.5),
    `75` = ~quantile(.x, 0.75),
    `97.5` = ~quantile(.x, 0.975)
    ), .names = "{.fn}"),
    .groups = "drop")


est_wb_rtrn |> 
  filter(between(year, 1998, 2022)) |> 
  ggplot(aes(year, rtrn)) +
  stat_lineribbon(aes(fill = mod), alpha = 0.4, show.legend = F) +
  geom_point(
    #data = coho_data_tbl |> filter(StockID == 161, between(year, 1998, 2022)),
    data = stan_data_list$rtrn |> filter(StockID==161),
    color = 1
  ) +
  facet_wrap(~mod) +
  geom_text(
    data = est_wb_rtrn_q |> filter(year == 2022) |> select(mod, year, rtrn = `50`),
    aes(label = scales::comma(round(rtrn))),
    nudge_x = 1.5
  ) +
  scale_fill_discrete(type = pal_lag_mod[1:2]) +
  scale_x_continuous("", breaks = 1998:2022, minor_breaks = 1998:2022, guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous("Forecast return", labels = scales::comma) +
  labs(title = "Willapa Bay preliminary 2022 forecast", 
       subtitle = "Posterior intervals of fits to 1998-2020 data",
       fill = "Model") +
  theme_light(base_size = 13)

##ggsave("O:/code/coho/forecast_wb/f_example_2022.png", width = 11, height = 7)

```

This chunk defines a convenience plotting function taking a *StockID* argument and returning time series plots paneled by model of the posterior distribution of estimated total return.

```{r ggfcst_ts}
ggfcst_ts <- function(stk = 161) {
  pdrw_stk <- filter(pdrw, StockID == stk, between(year, 1998, 2022))
  
  # #could add logic for function arg ar1only = T 
  # pdrw_stk <- filter(pdrw_stk, mod == "ar1") 
  
  ggplot(pdrw_stk, aes(year, rtrn)) +
    stat_lineribbon(aes(fill = mod), alpha = 0.4, show.legend = F) +
    geom_point(data = filter(stan_data_list$rtrn, StockID==stk), color = 1) +
    geom_text(
      data = filter(pdrw_stk, year == 2022) |> group_by(mod, year) |> summarise(rtrn = round(median(rtrn)), .groups = "drop"),
      aes(label = scales::comma(rtrn)),
      nudge_x = 1.5) +
    facet_wrap(~mod) +
    scale_fill_discrete(type = pal_lag_mod[1:2]) +
    #scale_x_continuous("", breaks = 1998:2022,  guide = guide_axis(n.dodge = 2)) +
    scale_x_continuous("", breaks = seq(1998, 2022, by = 2),  guide = guide_axis(angle = 45)) +
    scale_y_continuous("Forecast return", labels = scales::comma) +
    labs(title = paste(pdrw_stk$StockLongName[1], "2022 forecast"), 
         subtitle = "Posterior intervals of fits to data since 1998",
         fill = "Model") +
    theme_light(base_size = 13)
}

ggfcst_ts()
```

```{r other_stocks, eval=FALSE}
ggfcst_ts_coast <- map(
  coho_data_tbl |> distinct(StockID, StockLongName) |> filter(StockID > 120) |> pluck("StockID") |> set_names(), 
 ~ggfcst_ts(.x))

patchwork::wrap_plots(ggfcst_ts_coast)

ggfcst_ts(135) #Hoh
ggfcst_ts(131) #Quill Fall

ggfcst_ts_hc <- map(c(45, 51, 55, 59) |> set_names(), ~ggfcst_ts(.x))
patchwork::wrap_plots(ggfcst_ts_hc[c(1,3,4)], ncol = 1)

ggfcst_ts_jdf <- map(c(107, 111, 115, 117) |> set_names(), ~ggfcst_ts(.x))
patchwork::wrap_plots(ggfcst_ts_jdf)

ggfcst_ts_stsno <- map(c(29, 35) |> set_names(), ~ggfcst_ts(.x))
patchwork::wrap_plots(ggfcst_ts_stsno, ncol = 1)

bind_rows(fit_ar1_pred, fit_stipm_pred) |> 
  filter(StockID %in% c(29)) |> 
  select(year, mod, StockID, StockLongName, pop_id, `2.5%`,`25%`,`50%`,`75%`,`97.5%`) |> 
  pivot_wider(names_from = mod, values_from = `2.5%`:`97.5%`)

```

```{r posterior_density}
pdrw |> 
  filter(StockID == 161, year == 2022) |> 
  ggplot(aes(x = rtrn, color = mod, fill = mod)) + 
  stat_slab(alpha = 0.5) +
  geom_vline(
    data = filter(pdrw, StockID==161, year == 2022) |> group_by(mod) |> 
      summarise(across(rtrn, median), .groups = "drop"),
    aes(xintercept = rtrn, color = mod), size = 1.2,
    show.legend = F
  ) +
  geom_text(
    data = filter(pdrw, StockID==161, year == 2022) |> group_by(mod) |> 
      summarise(across(rtrn, median), .groups = "drop"),
    aes(x = rtrn, y = 1, label = scales::comma(round(rtrn)), color = mod),
    nudge_x = c(-12000, 12000),
    nudge_y = c(-0.02, 0.02),
    show.legend = F
  ) +
  scale_y_continuous("") +
  scale_x_continuous("Estimated return", labels = scales::comma) +
  scale_fill_discrete(type = pal_lag_mod[1:2], aesthetics = c("color", "fill")) +
  theme(legend.position = "top", legend.title = element_blank())
```

## Weighting

```{r simple_msa_weighted_median, eval=TRUE, results='show'}
#stack weighting for 2023...
#using weighting eqn in slide 31, with Oct 2021 lag-1 MSA

pdrw |>
  filter(StockID==161, year == 2022) |>
  group_by(mod) |>
  summarise(across(rtrn, median), .groups = "drop") |>
  mutate(
    w = 1/c(ar1 = 0.96, stipm = 0.65) / sum(1/c(ar1 = 0.96, stipm = 0.65)),
    rtrn_w = rtrn * w
  ) |> 
  summarise(oa3 = sum(rtrn_w), ja3 = oa3 * 1.2315) |> 
  mutate(across(everything(), ~scales::comma(round(.)))) |> 
  gt::gt()
  
#35776, 44059


# pdrw |>
#   filter(StockID==161, year == 2022) |>
#   group_by(mod) |>
#   summarise(across(rtrn, ~quantile(., p=seq(0.4, 0.6, by = 0.01))), .groups = "drop") |>
#   mutate(p = rep(seq(0.4, 0.6, by = 0.01), 2)) |>
#   #pivot_wider(names_from = mod, values_from = rtrn) |> print(n=30)
#   group_by(mod) |> slice_min(abs(35481 - rtrn), n = 1)
# 
# ## quick and dirty
# # bind_rows(fit_ar1_pred, fit_stipm_pred) |>
# #   filter(StockID==161) |>
# #   summarise(across(`50%`, ~mean(.)))
# # 
# # 35481*1.2315 #43695

```


## extract MS

```{r eval=FALSE}
# boot::inv.logit()
# or: function(x) { exp(x)/(1+exp(x)) }

# summary(fit_stipm, pars = "mu_mu_surv")$summary |>
#   as.data.frame() |> rownames_to_column("stan_out") |> tibble()

ms <- summary(fit_stipm, pars = "logit_smolt_survival_pop")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    across(`2.5%`:`97.5%`, ~(exp(.)/(1+exp(.)))),
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    yr = str_remove_all(stan_out, "logit_smolt_survival_pop\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[1])),
    year = yr + first(stan_data_list$y_min_max) - 1,
    pop_id = str_remove_all(stan_out, "logit_smolt_survival_pop\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[2])),
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

patchwork::wrap_plots(list(
  
ms |> filter(StockID==161) |> ggplot(aes(year, `50%`)) + geom_line() + geom_point() + geom_smooth(se = F) + scale_y_continuous("median posterior marine survival", labels = scales::percent) + labs(subtitle = "WB nat")

,
ms |> filter(StockID==161) |> 
  select(StockID, StockLongName, year, `2.5%`:`97.5%`) |> 
  pivot_longer(names_to = "q", values_to = "est", `2.5%`:`97.5%`) |>  
  ggplot(aes(q, est, color = q)) + 
  geom_boxplot(aes(fill = q), alpha = 0.4) + scale_y_continuous("median posterior marine survival", labels = scales::percent)
))

#EJDF and WJDF
ms |> filter(StockID %in% c(115,117)) |> ggplot(aes(year, `50%`, color = StockLongName)) + geom_line() + geom_point() + geom_smooth(se = F) + scale_y_continuous("median posterior marine survival", labels = scales::percent)

#Queets
ms |> filter(StockID==139) |> ggplot(aes(year, `50%`)) + geom_line() + geom_point() + geom_smooth(se = F) + scale_y_continuous("median posterior marine survival", labels = scales::percent) + labs(subtitle = "Queets nat")
```

```{r eval=FALSE}
#Coastal per requests, export "for consideration, use at your own risk" 
list(
  marine_surv = ms |> 
    filter(StockID > 125) |> 
    group_by(StockID, StockLongName, year) |> 
    summarise(across(c(n_eff, Rhat, `2.5%`:`97.5%`), median), .groups = "drop")
  ,
  rtrn = pdrw |> 
    filter(StockID > 125) |> 
    group_by(mod, StockID, StockLongName, year) |> 
    summarise(across(rtrn, median), .groups = "drop")
) |>
  writexl::write_xlsx("T:/DFW-Salmon Mgmt Modeling Team - General/Preseason/Coho/2022/forecasts/stipm_prelim_2022_estimates.xlsx")


ms |> 
  filter(StockID > 125) |> 
  ggplot(aes(year, `50%`, color = StockLongName)) + 
  geom_line(show.legend = T) + geom_point(show.legend = F) + geom_smooth(se = F, show.legend = F) + geom_hline(yintercept = 0.085) + 
  scale_y_continuous("median posterior marine survival", labels = scales::percent) + 
  #facet_wrap(~StockLongName, ncol = 1) +
  theme(text = element_text(size = 20))

```

## extract smolts

```{r eval=FALSE}
smolt <- summary(fit_stipm, pars = "smolt_est")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    #across(`2.5%`:`97.5%`, ~(exp(.)/(1+exp(.)))),
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    yr = str_remove_all(stan_out, "smolt_est\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[1])),
    year = yr + first(stan_data_list$y_min_max) - 1,
    pop_id = str_remove_all(stan_out, "smolt_est\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[2])),
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

smolt |> filter(StockID == 135)

```

