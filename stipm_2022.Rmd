---
title: "Willapa Bay natural coho forecast"
author: "Dan.Auerbach@dfw.wa.gov" 
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
output: 
  wdfwTemplates::wdfw_html_format

---

# Summary

This script presents a framework used by the Washington Department of Fish and Wildlife (WDFW) to generate preseason forecasts for Willapa Bay (WB) natural coho.

The work was described for a [2021 SSC review](https://www.pcouncil.org/documents/2021/10/f-1-attachment-3-a-proposed-forecast-methodology-for-natural-origin-willapa-bay-coho-o-kisutch-electronic-only.pdf/).

Estimated returns of WB natural coho (i.e., spawning escapement plus fishery catches) have shown substantial annual and longer-term fluctuations, posing a challenge to accurate preseason forecasts based on any method. For example, over the last decade, the population has exhibited both a major declining trend (from an average of ~108K in 2009-10 to ~18K in 2018-19), as well as dramatic year on year reversals (from ~96K in 2014 to ~19K in 2015). This difficult context heightens the importance of applying forecast methods that provide fishery managers with more than a single point estimate, and that can supply additional information to quantify relative risks of alternative decisions. 

# Setup

The following code readies the R environment (`r sessionInfo()$R.version$version.string`), calling necessary packages and loading an initial dataset.

```{r setup, results = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, warning = FALSE, message = FALSE, fig.width = 9, fig.height = 9)

library("tidyverse")
library("gt")
library("odbc"); library("DBI")
library("rstan")
options(mc.cores = 4)
rstan_options(auto_write = TRUE)
theme_set(theme_light())
library(tidybayes)
library(ggridges)

fp <- list(
  data_fram = "data/postseason_fram_spwn_hvst_ests.csv",
  data_cwt = "data/rmis_cwts.csv",
  data_smolt = "data/smolt_outmigrants.csv",
  data_full = "data/fram_cwt_smolt_fulljoin.csv"
  )

coho_data_tbl <- readr::read_csv(fp$data_full)


# ## dropped FRAM StockID 13 and 43, original pop_ids 10 and 24
# # fram_stocks <- c(105,93,89,45,51,55,75,81,61,23,149,63,107,115,111,157,97,135,153,101,69,1,85,139,131,127,145,11,17,59,35,29,117,161)
# 
# pop_meta <- readxl::read_excel("pop_meta.xlsx") |>
#   dplyr::select(pop_id:hat)
# #deleted and reindexed pop in file, no longer need: |> dplyr::filter(StockID != 13, StockID != 43)

sf_coord <- coho_data_tbl |> 
  distinct(StockID, StockLongName, pop_id, pop, lon, lat, hab_km) |> 
  sf::st_as_sf(coords = c("lon", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
  sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

#ar1, stipm, trailing arith mean, official preseason forecast
pal_lag_mod <- c("purple", "gold", "cyan", "pink") 

```

# Data

The analysis relies on per-stock escapement and harvest estimates compiled in post-season coho FRAM runs, as well as CWT-based marine survival estimates derived from RMIS, and smolt trap outmigrant estimates compiled by tribal comanagers and WDFW.

## FRAM postseason estimates of spawning escapement and harvest mortality

Coho FRAM tracks age 3 fish across 5 time steps corresponding to a calendar year. It includes unmarked and marked units of both natural and hatchery stocks. 

Data in the best available database (maintained by PSC CoTC) extend to 1986, but note that values prior to 1998 are of unknown origin.

```{r read_and_export_fram_tables, eval=FALSE}
#the relevant data are queried and re-exported as csv
mdb <- "O:/code/coho_fram_validation/PSC_CoTC_PostSeason_CohoFRAMDB_thru2019_021021.mdb"
#appears to be a mysterious RunID 3 in the Escapement and Mortality tables but not RunID table...
#aggregate mortality across timesteps for all sources to "estimated total harvest related impacts"
fram <- full_join(
  read_coho_escapement(mdb, stocks = pop_meta$FRAM_StockID) |> 
    filter(!is.na(RunYear))
  ,
  read_coho_mort(mdb, stocks = pop_meta$FRAM_StockID) |>
    filter(!is.na(RunYear)) |> 
    mutate(mort = LandedCatch + NonRetention + Shaker + DropOff + MSFLandedCatch + MSFNonRetention + MSFShaker + MSFDropOff) |> 
    group_by(RunYear, StockID, StockLongName) |> 
    summarise(mort = sum(mort), .groups = "drop")
  ,
  by = c("RunYear", "StockID", "StockLongName")
  ) |> 
  select(year = RunYear, StockID, StockLongName, spwn = escp, hvst = mort)

#fram |> filter(StockID == 161) |> print(n = 100)

write_csv(fram, "data/postseason_fram_spwn_hvst_ests.csv")

```

## Smolt outmigrant estimates

Smolt trap estimates are included from a dataset maintained by Marisa Litz (WDFW).

```{r request_to_ML, eval=FALSE}
#tbl_coho |> filter(!is.na(`Smolt Abundance`), year >= 1986) |> count(pop)

smolt_used <- stan_data_filter(coho_data_tbl, data_year_min = 1986, data_year_max = 2021, lag_spwn = 1, lag_hvst = 1) |> 
  getElement("smolt") 
#writexl::write_xlsx(smolt_used, "O:/code/coho/forecast_wb/smolt_outmig_to_update.xlsx")

smolt_used |> group_by(pop) |> summarise(ymin = min(year), ymax = max(year))

#original script drops numerous smolt outmigrant series that apparently could not be well reconciled to FRAM units
smolt_full_orig <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> 
  mutate(
    pop = case_when(
      !is.na(`SaSI Population`) ~ `SaSI Population`,
      is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
      is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
    )
  ) |> 
  select(1:8, pop) |> 
  #count(Smolt_Abundance_Matches_FRAM)
  #count(Smolt_Abunce_Matches_SASI_but_not_FRAM)
  filter(!is.na(`Smolt Abundance`), `Calendar Year` >= 1986)
#see "drop_pop"
smolt_used |> count(pop, smolt_ocn_surv_pop) #9 units
smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) |> print(n = 50) #23 units

left_join(
  smolt_used |> count(pop, smolt_ocn_surv_pop)
  ,
  smolt_full_orig |> count(`Managment Unit (FRAM)`, `Smolt Abundance Population`, pop) #|> print(n = 50)
  , by = c("pop" = "Managment Unit (FRAM)")
  ) |> 
  filter(!(pop.y %in% drop_pop)) |> 
  select(-contains("."))

```

Updated data are read, wrangled and re-exported in format congruent with FRAM units.

```{r smolt_update_checks_export, eval=FALSE}

smolt <- readxl::read_excel("O:/code/coho/forecast_wb/Smolt Time Series.xlsx", na = "NA") |> 
  select(
    year = OEY,
    Chehalis_149 = Chehalis,
    Deschutes_63 = Deschutes,
    Dungeness_107 = Dungeness,
    Green_97 = Green,
    Nisqually_69 = Nisqually,
    Nooksack_1 = Nooksack,
    Queets_139 = Queets.Clear,
    Skagit_17 = Skagit,
    Snohomish_35 = Snohomish) |> 
  filter(year >= 1986) |> 
  pivot_longer(cols = -year, names_to = "smoltpop_StockID", values_to = "smolt_update") |> 
  separate(smoltpop_StockID, into = c("smoltpop","StockID"), sep = "_") |> 
  mutate(StockID = as.numeric(StockID)) |> 
  filter(!is.na(smolt_update)) #excel wide format has complete cases, so reduce to stock-years with data


full_join(
  smolt_used |> select(pop_id, pop, StockID, year, smolt_orig = smolt)
  ,
  smolt
  ,
  by = c("StockID", "year")
  ) |> #214
  mutate(
    smolt = if_else(is.na(smolt_update), smolt_orig, smolt_update)
  ) |> 
  #filter(is.na(smolt_orig)) #16 updated values for 2018-2020
  #filter(round(smolt_orig) != round(smolt_update)) #1991 Chehalis does not match
  #filter(is.na(smolt_update)) #prior year vals not present in update, 3 Chehalis and 1 Nooksack
  select(StockID, year, smolt) |> 
  write_csv("data/smolt_outmigrants.csv")

```

## Marine survival CWT releases and recoveries

Estimates of marine survival are included via records of coded wire tagged fish releases and recoveries. This dataset was updated by Neala Kendall and Ty Garber, following fix of RMIS release count error for tagged wild units. (Note the original dataset was unaffected because 1) error not present in hatchery units 2) previous wild unit values drawn from underlying pre-RMIS data.)

```{r cwt_rec_rel_update_checks_export, eval=FALSE}
# #strings match and all 15 units used in analysis are present
# #reversing order of terms shows the 3 unused dataset units: Baker H, Minter Crk H and Satsop H
# setdiff(
#   filter(pop_meta, !is.na(smolt_ocn_surv_pop)) |> distinct(smolt_ocn_surv_pop),
#   readxl::read_excel("O:/code/coho/forecast_wb/coho SARs_to extend.xlsx",
#     sheet = "CWT_FRAM_Matches_complete202108") |>
#     distinct(smolt_ocn_surv_pop = `Smolt Ocean Survival Population`)
# )

cwt <- readxl::read_excel(
  "O:/code/coho/forecast_wb/coho SARs_to extend.xlsx", 
  sheet = "CWT_FRAM_Matches_complete202108"
  ) |> 
  select(
    smolt_ocn_surv_pop = `Smolt Ocean Survival Population`,
    pop = `Managment Unit (FRAM)`,
    year = `Calendar Year`, est_n_rec = Fishery_Plus_Escapement, est_n_rel = Release_No
  ) |> 
  left_join(
    pop_meta |> mutate(pop_id = as.numeric(factor(pop))),
    by = c("pop", "smolt_ocn_surv_pop")
    ) |> 
  #filter(is.na(pop_id)) |> print(n = 100)
  #filter(str_detect(pop, "Area 13")) |> print(n = 100)
  filter(!is.na(pop_id)) |> 
  select(StockID, pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)

# #compare against prior
# tbl_coho |> 
#   filter(pop_id == 36, between(year, 1986, 2020)) |> 
#   select(pop_id, pop, `Smolt Ocean Survival Population`, year, Fishery_Plus_Escapement, Release_No) |> print(n = 50)

# full_join(
#   coho_data_tbl |> select(pop_id, pop, smolt_ocn_surv_pop, year, est_n_rec, est_n_rel)
#   ,
#   cwt
#   ,
#   by = c("pop_id", "pop", "smolt_ocn_surv_pop", "year"), suffix = c("_orig", "")
#   ) |>
#   #filter(pop_id == 34, between(year, 1986, 2020)) |> print(n = 50)
#   mutate(
#     d_rec = est_n_rec - est_n_rec_orig,
#     d_rel = est_n_rel - est_n_rel_orig
#   ) |>
#   filter(
#     !is.na(d_rec), !is.na(d_rel),
#     (d_rec != 0 | d_rel != 0)) |>
#   select(pop_id:year, contains("_rel"), contains("_rec")) |> #print(n = 200)
#   writexl::write_xlsx("O:/code/coho/forecast_wb/coho_SARs_to_extend_diffs.xlsx")

cwt |> 
  filter(year >= 1986) |>
  select(StockID, year, est_n_rec, est_n_rel) |> 
  write_csv("data/rmis_cwts.csv")

```

## Combined into full dataset

This chunk demonstrates a tidy piped reproduction of the original data object.

```{r tbl_coho_full, eval=FALSE}
# #still retaining for now to avoid having to root through commits for any questions
# 
# drop_pop <- c("Queets", "Clearwater", 'Bell Creek', 'Johnson Creek', 'Jimmy Come Lately Creek',
#               'Deep Creek', 'McDonald Creek', 'Siebert Creek', 'Salt Creek', 'Discovery Bay',
#               'East Twin Creek', 'West Twin Creek', 'Northeast Hood Canal')
# 
# drop_cwt_surv <- c("Minter Crk H", "Baker H", "Satsop H") 
# 
# tbl_coho <- readr::read_csv(fp$coho_file, show_col_types = FALSE) |> #1709x21
#   mutate(
#     pop = case_when(
#       !is.na(`SaSI Population`) ~ `SaSI Population`,
#       is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Managment Unit (FRAM)`,
#       is.na(`SaSI Population`) & !is.na(SubPopulation) ~ SubPopulation
#     ),
#     spwn = case_when(
#       !is.na(`SaSI Population`) ~ `SASI Natural Origin Abundance`,
#       is.na(`SaSI Population`) & is.na(SubPopulation) ~ `Age 3 Escapement (FRAM)`,
#       is.na(`SaSI Population`) & !is.na(SubPopulation) ~ `SupPopulation Escapement`
#     ),
#     spwn = if_else(pop=="Discovery Bay", `SASI CompositeOrigin Abundance`, abs(spwn)) #corrects negative val for 2005 A12A Wild, orig L152
#     ,
#     hvst = spwn / (1/`Harvest (% FRAM)` - 1) 
#   ) |> 
#   filter( !(pop %in% drop_pop) ) |> 
#   select(`Calendar Year`, `Smolt Abundance`, Latitude, Longitude, pop, spwn, hvst) |> 
#   inner_join(
#     readr::read_csv(fp$stream_file, show_col_types = FALSE) |> 
#       filter(Population != "", !is.na(Population)) |> #no NAs in the current csv, but left for now
#       select(pop = Population, KM)
#     ,
#     by = "pop") |> 
#   full_join(
#     readr::read_csv(fp$cwt_file, show_col_types = FALSE) |> 
#       mutate(
#         hat = if_else(stringr::str_detect(`Smolt Ocean Survival Population`, " H$"), 1, 0)
#       ) |> 
#       filter( !(`Smolt Ocean Survival Population` %in% drop_cwt_surv)) |> 
#       select(pop = `Managment Unit (FRAM)`, `Calendar Year`, `Smolt Ocean Survival Population`, Fishery_Plus_Escapement, Release_No, hat)
#     ,
#     by = c("pop", "Calendar Year")) |> 
#   filter(`Calendar Year` > 1985) |> 
#   bind_rows(
#     tibble(`Calendar Year` = 2013, yr = 28, pop = "Area 7-7A Independent Wild"),
#     tibble(`Calendar Year` = 2004, yr = 19, pop = "Port Gamble Bay Wild"),
#     tibble(`Calendar Year` = 2000:2001, yr = 15:16, pop = "Grays Harbor Miscellaneous Wild")
#   ) |>
#   left_join(
#     bind_rows(
#       tibble(pop = "Green River Wild", Long = -122.2145, Lat = 47.3519),
#       tibble(pop = "Area 10E Miscellaneous Wild", Long = -122.8242, Lat = 47.5896)
#     )
#     , by = "pop") |>
#   mutate(
#     Longitude = if_else(is.na(Long), Longitude, Long), Long = NULL,
#     Latitude = if_else(is.na(Lat), Latitude, Lat), Lat = NULL,
#     Basin = if_else(Longitude < -123.80, 0, 1),
#     `Smolt Abundance` = if_else(pop == "Puyallup River Wild", NA_real_, `Smolt Abundance`),
#     hvst = if_else(stringr::str_detect(pop, "Quillayute River") & `Calendar Year` <= 1987, NA_real_, hvst),
#     pop_id = as.numeric(factor(pop)),
#     yr = `Calendar Year` - min(`Calendar Year`) + 1
#   ) |> 
#   rename(year = `Calendar Year`) |> 
#   arrange(pop)
# 
# sf_coord <- tbl_coho |> 
#   group_by(pop, pop_id) |> 
#   summarise(long = median(Longitude, na.rm = T), lat = median(Latitude, na.rm = T), .groups = "drop") |> 
#   sf::st_as_sf(coords = c("long", 'lat'), crs = sf::st_crs("+proj=longlat +datum=WGS84")) |> 
#   sf::st_transform(crs = sf::st_crs("+proj=utm +zone=10T ellps=WGS84"))

```

This chunk performs the joins to generate the full dataset used in the current analysis.

```{r updating_full_dataset, eval=FALSE}
#Note the original negative escapement in A12A Wild in 2005 is due to an error in the FRAM database
#so still needs correcting: spwn is coerced to 0 here, but harvest is unaffected b/c drawn directly from Mortality
#note also Grays Hbr Misc Wild missing 2000 & 2001 in FRAM mdb and Area 13A Miscellaneous Wild missing 1996
#both fixed with simple insertion of NAs

#here the meta identifiers are (expanding left) joined to the FRAM data
#after these have been full joined to full joined CWT and smolt data

coho_data_tbl <- pop_meta |>
  full_join(
    full_join( #FRAM + (CWT + smolt)
      read_csv(fp$data_fram) |>
        select(-StockLongName) |>
        bind_rows(tibble(year = c(1996, 2000:2001), StockID = c(81, 157, 157))) |> 
        mutate(
          spwn = if_else(spwn < 0, 0.01, spwn),
          rtrn = spwn + hvst
        ) |>  
        arrange(StockID, year) #1156, 1986:2019
      ,
      full_join(
        read_csv(fp$data_cwt), #396
        read_csv(fp$data_smolt), #214 
        by = c("StockID", "year")
        )
      ,
      by = c("StockID", "year")
      )
    ,
    by = c("StockID"))

write_csv(coho_data_tbl, "data/fram_cwt_smolt_fulljoin.csv")

# coho_data_tbl |> count(StockID, pop_id, pop) |> print(n = 100)
# 
# #WB has FRAM through 2019, hatchery-based rel/rec through 2018, no smolt
# coho_data_tbl |> filter(StockID == 161) |> print(n = 50)
# 
# #Chehalis has the Bingham Crk numbers
# coho_data_tbl |> filter(StockID == 149) |> print(n = 50)
# 
# #Skagit is an example of "max data": FRAM to 2019, CWT to 2018, smolt to 2020
# coho_data_tbl |> filter(StockID == 17) |> print(n = 50)

```

## TEMP adding 2022 updates

```{r add_prelim_spwn}
#Pre-CoTC 2020 escapement estimates that feed postseason BackwardsFRAM table. 
#Not identical to forward FRAM resulting Escapement table values used in other years, but 1) should be close and 2) are what is available.

coho_data_tbl_2020 <- rows_upsert(
  coho_data_tbl
  ,
  left_join(
    coho_data_tbl |> 
      distinct(pop_id, pop, StockID, StockLongName, lon, lat, hab_km, smolt_ocn_surv_pop, hat) |> 
      mutate(year = 2020)
    ,
    readxl::read_excel(
      "T:/DFW-Salmon Mgmt Modeling Team - General/Escapement files/Coho/fram_coho_escapement_2022.xlsx", 
      range = "BKFRAM!A4:Y168", col_types = c("numeric", "text", rep("numeric", length(1998:2020)))) |> 
      select(StockID = `Row Labels`, spwn = `2020`) |> 
      mutate(year = 2020)
    ,
    by = c("StockID", "year")
  )
  ,
  by = c("StockID", "year")
)

# coho_data_tbl |> filter(year >= 2020) |> print(n=50)
# coho_data_tbl_2020 |> filter(year >= 2020) |> print(n=50)

```

```{r add_new_smolt}
##Jan 20 2022 email from Dr. Litz: "O:/code/coho/forecast_wb/Smolt Time Series Data.csv"
## initiated new sheet 
# coho_data_tbl |>
#   filter(!is.na(smolt)) |>
#   select(year, StockID, StockLongName, smolt) |>
#   arrange(year, StockID) |>
#   write.csv("clipboard", row.names = F)

#manually copied a few relevant 2021 values from "O:/code/coho/forecast_wb/Smolt Time Series Data.csv"
#updated Nooksack 2018-2020
#added Chehalis (backcalc'd) and Queets+Clearwater from email text
#no 2020 for Sno, Deschutes, Chehalis

coho_data_tbl_2020 <- rows_upsert(
  coho_data_tbl_2020
  ,
  #here have prior years, so append id cols, reversing pattern for escp
  left_join(
    readxl::read_excel("O:/code/coho/forecast_wb/Smolt Time Series.xlsx", sheet = "stipm_2022") |> 
      select(year, StockID, smolt) |> 
      filter(year > 2017)
    ,
    coho_data_tbl |> 
      distinct(pop_id, pop, StockID, StockLongName, lon, lat, hab_km, smolt_ocn_surv_pop, hat)
    ,
    by = c("StockID"))
  ,
  by = c("StockID", "year")) 

# coho_data_tbl_2020 |> filter(year >= 2020) |> print(n=50)

```

Construct a preliminary harvest from simple regression on escapement

```{r}
coho_data_tbl |> filter(StockID==161, year>1997) |> ggplot(aes(year, hvst)) + geom_col()
filter(coho_data_tbl, StockID==161, year>1997) |> ggplot(aes(year, hvst/spwn)) + geom_col()
filter(coho_data_tbl, StockID==161, year>1997) |> ggplot(aes(year, hvst/(hvst+spwn))) + geom_col()

coho_data_tbl |> filter(StockID==161, year>1997) |> ggplot(aes(spwn, hvst, color = year)) + geom_point() + scale_x_continuous(limits = c(0,100000)) + scale_y_continuous(limits = c(0,100000))

coho_data_tbl |> filter(StockID==161, year>1997) |> ggplot(aes(log(spwn), log(hvst), color = year, label = year)) + geom_point() + geom_text() + geom_smooth(method = "lm")# + facet_wrap(~year>2015)

summary(lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl, StockID==161, year>1997)))
#tightening to recent years in lower return regime (also boosts r2) 
summary(lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl, StockID==161, year>=2016)))
predict(
  lm(log(hvst) ~ log(spwn), data = filter(coho_data_tbl, StockID==161, between(year, 2016, 2019))),
  filter(coho_data_tbl_2020, StockID==161, year==2020)
  ) |> exp() #6270
```


```{r}
# #add just WB
coho_data_tbl_2020 <- rows_upsert(
  coho_data_tbl_2020
  ,
  filter(coho_data_tbl_2020, StockID==161, year==2020) |>
    mutate(hvst = 6270, rtrn = spwn + hvst)
  ,
  by = c("StockID", "year"))

## OR add stocks with decent correlations and r2 in recent years
coho_data_tbl_2020 <- rows_upsert(
  coho_data_tbl_2020
  ,
  coho_data_tbl |> 
    filter(between(year, 2016, 2019)) |> 
    group_by(StockID, StockLongName) |> nest() |> #very similar to nest_by
    summarise(
      corr = map_dbl(data, ~cor(log(.x$hvst), log(.x$spwn))),
      fit = map(data, ~lm(log(hvst) ~ log(spwn), data = .x)),
      .groups = "keep"
    ) |> 
    left_join(
      filter(coho_data_tbl_2020, year == 2020) |> select(StockID, StockLongName, spwn)
      , by = c("StockID", "StockLongName")
    ) |> 
    nest(newdata = spwn) |> 
    mutate(
      r2 = map_dbl(fit, ~summary(.x) |> pluck("r.squared")),
      hvst = map2_dbl(fit, newdata, ~predict.lm(.x, newdata = .y) |> exp())
    ) |>
    unnest(newdata) |> ungroup() |> 
    arrange(desc(r2, corr)) |> #print(n = 50)
    filter(corr >= 0.8, r2 > 0.8, !is.na(hvst)) |> 
    mutate(rtrn = spwn + hvst, year = 2020) |> 
    select(StockID, StockLongName, year, spwn, hvst, rtrn)  
  ,
  by = c("StockID", "year"))

    
```


\

# Model fitting

Models are first fit to progressive subsets of the data filtered by year in order to gauge *as applied* forecast skill. Rather than a "sliding window" (e.g., the prior 10 years stepped forward each year), the dataset is "stretched" from a fixed starting year.

The data available for the 2022 preseason forecast are:

 - Smolt outmigrant abundances from 2020 or 2021
 - CWT recoveries & releases (informing marine survival) from 2019 or 2020; from 2017 brood, released in 2019 with recoveries in 2020
 - FRAM-based estimates of harvest and escapement (informing annual total return) from 2019 or 2020
  - 2019 values are complete from the previous preseason planning process
  - 2020 estimates are being compiled for Jan/Feb CoTC post-season runs
  - preliminary 2021 escapement estimates may be available, but will still be in review and undergoing regional QAQC
    

## `stan` functions

A helper function trims years from the complete dataset according to arguments controlling the data-type-specific lag from a data `data_year_max` corresponding to the year before the desired forecast year (i.e., a `data_year_max` of 2021 designates the most recent possible data for the 2022 forecast). 

```{r stan_data_filter}
#trim years then declare separate intermediaries
#"data_year_max" is "last/max year of available data", so desired year_pred-1
#lags allow for flexible OAT configurations with additional trimming
#but can be zeroed to allow all available data
#for a 2022 forecast in adult_pred vectors from stan
#data_year_max is 2021, s.t. firmly available postseason FRAM is 2019 
stan_data_filter <- function(
  full, #complete dataset
  data_year_min, #starting anchor year
  #in STIPM, adult_pred[data_year_max+1] is a total return from smolt[data_year_max]*surv[data_year_max], with adult_est[data_year_max] as the spawning escapement
  #in AR1, adult_est and adult_pred are total return, and adult_pred[year_pred] is same as last year of adult_est b/c passing n_year+1 relative to STIPM
  data_year_max,
  lag_smolt = 0, #n-extra-years trimmed from smolt abundances 
  lag_MS = 1, #n-extra-years trimmed from release and recovery data
  lag_spwn = 2, #n-extra-years trimmed from FRAM escapement
  lag_hvst = 2 #n-extra-years trimmed from FRAM harvest
  ){
  
  full_ymin_ymax = filter(full, between(year, data_year_min, data_year_max)) |> 
    arrange(pop_id, year) |>  #ensure everything sorted to prevent slice point mishaps
    mutate(yr = year - min(year) + 1)  #reindexing for stan
  
  stan_data = list(
    y_min_max = data_year_min:data_year_max,
    smolt = filter(full_ymin_ymax, !is.na(smolt), year <= data_year_max - lag_smolt),
    MS = filter(full_ymin_ymax, !is.na(est_n_rec), year <= data_year_max - lag_MS),
    spwn = filter(full_ymin_ymax, !is.na(spwn), year <= data_year_max - lag_spwn),
    hvst = filter(full_ymin_ymax, !is.na(hvst), year <= data_year_max - lag_hvst),
    rtrn = filter(full_ymin_ymax, !is.na(rtrn), year <= data_year_max - max(c(lag_spwn, lag_hvst)))
  )
  
  return(stan_data)
}

# #tests, random start year
# stan_data_filter(coho_data_tbl, 2004, 2006) |> map(tail) #predicting 2007, with defaults only get FRAM through 2004
# stan_data_filter(coho_data_tbl, 2004, 2007) |> map(tail) #predicting 2008, defaults give FRAM 2005

```

Next, wrappers to the `rstan::stan` function facilitate repeated fitting with consistent control parameters and input data lists.

```{r ar1_functions}
stan_ar1 <- function(stan_data, n_iter = 200, n_chain = 2){
  #note following orig naming convention where "tot" refers to "total return" = spwn + hvst = rtrn, as estimated from FRAM
  stan_fit <- stan(
    file = 'LD_coho_forecast_AR_ind_2_v4.stan',
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.999, max_treedepth = 12),
    data = list(
      n_year = length(stan_data$y_min_max) + 1, #sets year of adult_pred and year-dim of adult_est 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      n_pop_tot = length(unique(stan_data$rtrn$pop_id)), #Number of populations with return data
      pop_tot = unique(stan_data$rtrn$pop_id), #Which populations possess return data
      n_tot = length(stan_data$rtrn$rtrn), #Length of the return data vectors
      tot_dat = stan_data$rtrn$rtrn,  #Vectors of all return data across all populations
      tot_true = stan_data$rtrn$yr, #Vectors of the indices identifying which years are those with non-NA data for the return data
      #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
      slice_tot_start = pmatch(unique(stan_data$rtrn$pop_id), stan_data$rtrn$pop_id), #stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_tot_end = c(tail(pmatch(unique(stan_data$rtrn$pop_id), stan_data$rtrn$pop_id)-1, -1), length(stan_data$rtrn$pop_id)) #stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  
  return(stan_fit)
}

```

```{r stipm_functions}
stan_stipm <- function(stan_data, n_iter = 200, n_chain = 2){
  #note n_year does not include "+1" of AR1 since adult_pred is calc'd separately as
  # smolt[n_year]*surv[n_year], thereby giving adult run size (spwn+hvst) in n_year+1
  #stan_data$y_min_max accordingly is first:last_year_of_data
  stan_fit <- stan(
    file = 'LD_coho_forecast_6_2_4.stan', 
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.99, max_treedepth = 10.25),
    data = list(
      n_year = length(stan_data$y_min_max), 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      u = matrix(1, nrow = 1, ncol = length(unique(stan_data$spwn$pop_id))),
      dist = units::drop_units(sf::st_distance(sf_coord)/10000), #values are identical
      
      pop_smolt = unique(stan_data$smolt$pop_id), #pop_ids with smolt data 
      n_pop_smolt = length(unique(stan_data$smolt$pop_id)),
      smolt_true = stan_data$smolt$yr,
      smolt_dat = stan_data$smolt$smolt,
      n_smolt = nrow(stan_data$smolt),
      
      pop_esc = unique(stan_data$spwn$pop_id),  # pop_ids with escapement data
      n_pop_esc = length(unique(stan_data$spwn$pop_id)),
      esc_true = stan_data$spwn$yr,
      esc_dat = stan_data$spwn$spwn,
      n_esc = nrow(stan_data$spwn),
      
      pop_catch = unique(stan_data$hvst$pop_id), #pop_ids with harvest data
      n_pop_catch = length(unique(stan_data$hvst$pop_id)),
      harvest_true = stan_data$hvst$yr,
      harvest_dat = stan_data$hvst$hvst,
      n_harvest = nrow(stan_data$hvst),
      
      pop_MS = unique(stan_data$MS$pop_id), #pop_ids with marine survival data
      n_pop_MS = length(unique(stan_data$MS$pop_id)),
      MS_true = stan_data$MS$yr, #orig uses Fishery_Plus_Escapement rather than Release_No to filter...
      MS_dat_x = stan_data$MS$est_n_rec |> round() |> as.integer(), #stan expects integer; previously labeled Fishery_Plus_Escapement
      MS_dat_N = stan_data$MS$est_n_rel |> round() |> as.integer(), #previously labeled Release_No
      n_MS = nrow(stan_data$MS),
      
      stream_dist = stan_data$spwn |> distinct(pop, hab_km) |> pluck("hab_km"),
      
      sigma_esc = 0.2,
      
      n_hatchery = filter(stan_data$MS, hat == 1) |> distinct(pop_id) |> nrow(),
      hatchery = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat > 0, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),
      wild = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat < 1, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),

      slice_smolt_start = pmatch(unique(stan_data$smolt$pop_id), stan_data$smolt$pop_id), #stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_smolt_end = c(tail(pmatch(unique(stan_data$smolt$pop_id), stan_data$smolt$pop_id)-1, -1), length(stan_data$smolt$pop_id)), #stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_esc_start = pmatch(unique(stan_data$spwn$pop_id), stan_data$spwn$pop_id), #stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_esc_end = c(tail(pmatch(unique(stan_data$spwn$pop_id), stan_data$spwn$pop_id)-1, -1), length(stan_data$spwn$pop_id)), #stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_harvest_start = pmatch(unique(stan_data$hvst$pop_id), stan_data$hvst$pop_id), #stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_harvest_end = c(tail(pmatch(unique(stan_data$hvst$pop_id), stan_data$hvst$pop_id)-1, -1), length(stan_data$hvst$pop_id)), #stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_MS_start = pmatch(unique(stan_data$MS$pop_id), stan_data$MS$pop_id), #stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_MS_end = c(tail(pmatch(unique(stan_data$MS$pop_id), stan_data$MS$pop_id)-1, -1), length(stan_data$MS$pop_id)) #stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  return(stan_fit)
}
```

## One-ahead *post-hoc* forecast

The one-at-a-time model fitting proceeds by iterating through a vector of "max data years", predicting the following year by first filtering down a subset of the estimation data, then calling the stan models, and then extracting diagnostic and posterior results.

```{r oat, eval=FALSE}
#intermediary single year output is saved out within map() in case of loop disruptions
#a dataset data_year_max of 2014, predicting 2015, with default lags of 2 gives FRAM/spwn through 2012
#but with "compiling" lags of 1 gives FRAM/spwn through 2013

oat_wrap <- function(iter, #3K 
                     chain, #4
                     oat_data_year_min, #1998,
                     oat_years_max, #2008:2018, #max data year, predicting 2009:2019
                     lag_spwn, lag_hvst, lag_MS, lag_smolt #deducted from data year
                     ){
  #build a list named by max data year 
  oat <- set_names(oat_years_max) |> 
    map(function(x) {
      print(paste("max data year", x, "predicting", x+1))
      stan_data_list <- stan_data_filter(coho_data_tbl, data_year_min = oat_data_year_min, data_year_max = x, lag_spwn = lag_spwn, lag_hvst = lag_hvst, lag_MS = lag_MS, lag_smolt = lag_smolt) 
      print(stan_data_list$y_min_max)
      print(c("smolt", range(stan_data_list$smolt$year)))
      print(c("MS", range(stan_data_list$MS$year)))
      print(c("spwn", range(stan_data_list$spwn$year)))
      print(c("hvst", range(stan_data_list$hvst$year)))
      print(c("rtrn", range(stan_data_list$rtrn$year)))

      print(paste("AR1 start", Sys.time()))

      fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall AR1 stan data arg n_year+1 relative to STIPM gives adult_pred in desired year
      fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "ar1",
          n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      print(paste("STIPM start", Sys.time()))

      fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
      fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "stipm",
          n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      pred_out <- bind_rows(fit_ar1_pred, fit_stipm_pred)
      saveRDS(pred_out, paste0("oat_fits/oat_pred_",x+1,"_lags_spwn",lag_spwn,"_hvst",lag_hvst,"_MS",lag_MS,"_smolt",lag_smolt,".rds"))
      return(pred_out) #list element for max-data-year-x 
    })
  return(oat)
}

#predicting 2009:2019; renaming objects with lags relative to prediction year (rather than data year, as in args)
oat_lag3 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 2, lag_hvst = 2, lag_MS = 1, lag_smolt = 0)
oat_lag2 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 1, lag_hvst = 1, lag_MS = 1, lag_smolt = 0)
oat_lag1 <- oat_wrap(iter = 3000, chain = 4, oat_data_year_min = 1998, oat_years_max = 2008:2018, lag_spwn = 0, lag_hvst = 0, lag_MS = 1, lag_smolt = 0)

```

# Results

Saved output from the one-ahead runs is read back into memory and joined with observations to allow examination of performance skill.

```{r read_bind_oat_fits, eval=FALSE}
# #tibbles of annual predictions for AR1 & STIPM with a given lag
# bind_rows(
#   list.files("oat_fits", pattern = "lags_spwn0_hvst0", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l1_", mod))
#   ,
#   list.files("oat_fits", pattern = "lags_spwn1_hvst1", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l2_", mod))
#   ,
#   list.files("oat_fits", pattern = "lags_spwn2_hvst2", full.names = T) |>
#     map_df(~readRDS(.x)) |> mutate(lag_mod = paste0("l3_", mod))
# ) |>
#   select(lag_mod, year, StockID, pop_id, pop, n_diverg, n_eff, Rhat, `2.5%`:`97.5%`) |>
#   saveRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

```

For comparison, generate deterministic trailing means and read official preseason forecast compilation.

```{r trailing_means_and_fcst}
#create a temp col of the best case values that would be available in a given forecast year (i.e., up to year - 1)
#then create rolling mean over prior values
#in addition, join the compilation of submitted forecasts from Marlene Bellman
rtrn_trail_mean_fcst <- coho_data_tbl |> 
  select(StockID, pop_id, pop, year, rtrn) |>
  group_by(StockID, pop_id, pop) |> 
  mutate(
    lag3_rtrn = c(NA, NA, NA, head(rtrn, -3)),
    l3_trail_mean = slider::slide_dbl(lag3_rtrn, ~mean(., na.rm=T), .before = 2),
    lag2_rtrn = c(NA, NA, head(rtrn, -2)),
    l2_trail_mean = slider::slide_dbl(lag2_rtrn, ~mean(., na.rm=T), .before = 2),
    lag1_rtrn = c(NA, head(rtrn, -1)),
    l1_trail_mean = slider::slide_dbl(lag1_rtrn, ~mean(., na.rm=T), .before = 2)
  ) |> 
  ungroup() |> 
  left_join(
    #forecast of record compilation; not public
    read_csv("O:/code/coho/forecast_wb/ForecastCompilation_SourceDataPre_MB080520.csv") |> 
      select(StockID, year = YEAR, pre_fcst = RunSize)
    , by = c("year", "StockID")
  ) |> 
  filter(between(year, 2009, 2019)) |> select(-starts_with("lag")) |>  
  pivot_longer(cols = c(contains("trail"), pre_fcst), names_to = "lag_mod", values_to = "50%") 

```

These are bound together and performance measures are calculated.

```{r read_oat_and_oat_obs}
oat <- readRDS("oat_fits/oat_l0_l1_l2_2009_2019.rds")

(
oat_obs <- left_join(
  bind_rows(
    oat |> select(lag_mod, year, StockID, pop_id, pop, `2.5%`:`97.5%`),
    rtrn_trail_mean_fcst |> select(-rtrn) #will add back in next join
    ),
  coho_data_tbl |> 
    select(year, StockID, rtrn) |> 
    group_by(StockID) |>  #adding MASE denominator
    mutate(scale_err = mean(abs(rtrn - c(NA, head(rtrn, -1))), na.rm = T)) |> 
    ungroup(),
  by = c("year", "StockID")
  ) |>
  mutate(
    in_50 = (rtrn >= `25%`) & (rtrn <= `75%`),
    in_95 = (rtrn >= `2.5%`) & (rtrn <= `97.5%`),
    #convention: actual - forecast
    #possibly counterintuitive, but
    #negative is overforecast ("fewer returned than predicted"),
    #positive is underforecast ("more returned than predicted")  
    err = rtrn - `50%`,
    err_log = log(rtrn) - log(`50%`),
    err_abs_pct = abs(err / rtrn),
    lar = log(`50%`/rtrn), #log accuracy ratio, used for median symmetric accuracy as described in Morley et al 2018
    ase = abs(err) / scale_err #absolute scaled error for MASE
  ) |> 
  arrange(year, StockID, lag_mod)
)


#oat |> count(lag_mod) ; oat_obs |> count(lag_mod) 
#oat_obs |> select(lag_mod, err:ase) |> summary()
```


## Diagnostics

```{r oat_rhat_neff_n_diverg}
# #all years, all pops
# oat |> 
#   group_by(lag_mod) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop")

# #by year, all pops
# oat |> 
#   group_by(lag_mod, year) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop") |> 
#   #  print(n = 100)
#   gt::gt(rowname_col = "lag_mod") |>
#   gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |>
#   gt::fmt_number(contains("Rhat"), decimals = 2) |>
#   gt::fmt_number(contains("n_eff"), decimals = 0)

#Willapa, all years, similar to "all pops"
oat |> 
  filter(StockID == 161) |> 
  # arrange(lag_mod, year) |>
  # #filter(str_detect(lag_mod, "stipm")) |> 
  # print(n = 100)
  group_by(lag_mod) |> 
  summarise(
    n_diverg = max(n_diverg),
    Rhat_med = median(Rhat), Rhat_max = max(Rhat),
    n_eff_med = median(n_eff), n_eff_min = min(n_eff),
    .groups = "drop") |> 
  gt::gt(rowname_col = "lag_mod") |> 
#  gt::cols_hide(c("pop_id", "pop")) |> 
  gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "2009-19, Willapa Bay Natural") |> 
  gt::fmt_number(contains("Rhat"), decimals = 2) |> 
  gt::fmt_number(contains("n_eff"), decimals = 0)

# #n_eff boxes
# oat |> 
#   filter(StockID == 161, str_detect(lag_mod, "stipm")) |> 
#   ggplot(aes(lag_mod, n_eff)) + geom_boxplot() + geom_jitter(width = 0.1)
# #n_eff col timeseries, arguably slightly better after 2012
# oat |> 
#   filter(StockID == 161, str_detect(lag_mod, "stipm")) |> 
#   ggplot(aes(factor(year), n_eff, fill = lag_mod)) + geom_col(position = "dodge") + scale_fill_grey()

```

Note that additional revision of the AR1 `stan` code ("_v4" in the repository) has further improved sampling diagnostics.

## One-ahead Performance

### Lag3 paneled by year

Posterior medians and 95% CIs are shown for the AR1 (purple) and STIPM (gold) for each individual forecast year relative to the filtered data subset (darker lines) and the subsequent return observations. Also shown are the deterministic lagged trailing mean (cyan) and the submitted forecast of record (pink).

```{r oat_obs_pred_patchwork, fig.width=8, fig.height=10}
# set_names(2008:2018) |> 
#   map(function(x) {
#     d <- coho_data_tbl |> filter(StockID == 161, between(year, 1998, x)) |> select(year, StockID, pop, rtrn) |> mutate(year = as.character(year))
#     # d <- stan_data_filter(coho_data_tbl, data_year_min = 1998, data_year_max = x, lag_spwn = 2, lag_hvst = 2, lag_MS = 1, lag_smolt = 0) |> 
#     #    pluck("rtrn") |> filter(StockID == 161) |> select(year, StockID, pop, rtrn) |> mutate(year = as.character(year))
#     
#     oat_obs |> 
#       filter(StockID == 161, str_detect(lag_mod, "l3"), year == x+1) |> mutate(year = as.character(year)) |> 
#       ggplot() +
#       #all pre-forecast data
#       geom_col(data = d, aes(x = year, y = rtrn), fill = grey(0.8), width = 0.5) +
#       #available data for L3
#       geom_col(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), fill = grey(0.4), width = 0.5) +
#       #observed in forecast year
#       geom_point(aes(x = year, y = rtrn), color = 1, shape = 17)  +
#       #forecasts
#       geom_pointrange(aes(x = year, y = `50%`, ymin = `25%`, ymax = `75%`, color = lag_mod), fatten = 1.1, position = position_dodge(width = 0.5), show.legend = F) +
#       geom_point(aes(x = year, y = `50%`, color = lag_mod), position = position_dodge(width = 0.5), show.legend = F) +
#       geom_vline(xintercept = as.character(x-2), linetype = "dotted", size = 1) +
#       scale_x_discrete(name = "", limits = as.character(1998:2020), drop = FALSE) +
#       scale_y_continuous(name = "Return", limits = c(0,125000), labels = scales::comma) + #limits drops point error when outside ymax
#       scale_color_manual(name = "", values = pal_lag_mod) +
#       theme(axis.text = element_text(size = 6), axis.title = element_text(size = 5))
#     #+ labs(subtitle = paste("Predicting", x+1))
# }) |> 
#   patchwork::wrap_plots(ncol = 1)


set_names(2008:2018) |> 
  map(function(x) {
    d <- coho_data_tbl |> filter(StockID == 161, between(year, 1998, x)) |> select(year, StockID, pop, rtrn)

    oat_obs |> 
      filter(StockID == 161, str_detect(lag_mod, "l3|pre_fcst"), year == x+1) |> 
      ggplot() +
      #all pre-forecast data
      geom_line(data = d, aes(x = year, y = rtrn), color = grey(0.8), size = 0.9) +
      geom_point(data = d, aes(x = year, y = rtrn), color = grey(0.8), size = 0.9) +
      #available data for L3
      geom_line(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), color = grey(0.4), size = 0.7) +
      geom_point(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), color = grey(0.4), size = 0.7) +
      #observed in forecast year
      geom_point(aes(x = year, y = rtrn), color = 1, shape = 17)  +
      #forecasts
      geom_pointrange(aes(x = year, y = `50%`, ymin = `2.5%`, ymax = `97.5%`, color = lag_mod), fatten = 1.02, position = position_dodge(width = 0.5), show.legend = F) +
      geom_point(aes(x = year, y = `50%`, color = lag_mod), size = 0.9, position = position_dodge(width = 0.5), show.legend = F) +
      geom_vline(xintercept = x-2, linetype = "dotted", size = 1) +
      scale_x_continuous(name = "", limits = c(1998, 2020), breaks = 1998:2020, labels = 1998:2020) +
#      scale_y_continuous(name = "Return", limits = c(0,350000), labels = scales::comma) + #limits drops point error when outside ymax
      scale_y_continuous(name = "Return", labels = scales::comma) + #limits drops point error when outside ymax
      scale_color_manual(name = "", values = pal_lag_mod) +
      theme(axis.text = element_text(size = 6), axis.title = element_text(size = 5))

}) |> 
  patchwork::wrap_plots(ncol = 1)

ggsave("O:/code/coho/forecast_wb/f_oat_l3_by_pred_year.png", width = 7, height = 9)
```

### Lag3 50th with ribbons

The panels of years in the prior plot are collapsed, with ribbons illustrating the 50th and 95% CI. 

```{r l3_50th_ribbons, fig.width=8, fig.height=8}
oat_ribbon <- function(stk = 161, lg_md = "l3"){
  d <- oat_obs |> filter(StockID == stk, str_detect(lag_mod, lg_md))
  g <- ggplot(d, aes(year)) + 
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_point(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn)) +
    geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`, color = lag_mod, fill = lag_mod), alpha = 0.2, linetype = 0) + 
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`, color = lag_mod, fill = lag_mod), alpha = 0.4, linetype = 0) + 
    geom_line(aes(y = `50%`, color = lag_mod), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", labels = scales::comma) +
    scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
    theme(legend.position = "top") +
    labs(title = d$pop[1])
  return(g)
}

g <- oat_ribbon(lg_md = "l3|pre_fcst")
g + facet_wrap(~lag_mod)
#ggsave("O:/code/coho/forecast_wb/f_oat_l3_pred_single_panel.png", width = 11, height = 9)

# oat_ribbon(131) #Quill Fall, accurate
# oat_ribbon(153) #Hump, STIPM very good in later years
# oat_ribbon(111) #Elwha, wacky
# oat_ribbon(157) #GH, bouncy
# oat_ribbon(35) #Snohomish, STIPM def looks better but MSA worst...

```

### Rank order of WB returns 1998:2019

Plotting annual returns ordered by magnitude can provide a useful perspective on how recent years have compared to the longer series (Note this figure is not used in the submission document).

```{r wb_rtrn_rank_order, fig.width=6, fig.height=5}
coho_data_tbl |> 
  filter(StockID == 161, between(year, 1998, 2019)) |> 
  mutate(year = factor(year)) |> 
  ggplot(aes(fct_reorder(year, rtrn, min, .desc = T), rtrn)) +
  geom_col() +
  scale_x_discrete("Return year ordered by magnitude", guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(name = "Return", labels = scales::comma)

#ggsave("O:/code/coho/forecast_wb/f_wb_return_rank_ordered.png", width = 6, height = 4)
```

### Lag3 error time series

The raw prediction error (observed - forecast) is shown for the state space models alongside the deterministic mean and the recorded FRAM forecast.

```{r l3_error_series, fig.width=7, fig.height=7}
ggdata <- oat_obs |> 
  filter(StockID == 161, str_detect(lag_mod, "l3|pre_fcst")) |> 
  arrange(year, lag_mod) |> 
  mutate(year = as.character(year))

ggplot(ggdata, aes(year)) + 
  geom_col(aes(y = err, color = lag_mod, fill = lag_mod), position = position_dodge(), width = 0.7) +
  annotate("text", "2013", 8e4, label = "Underforecast:\n more returned than expected") +
  annotate("text", "2017", -5.5e4, label = "Overforecast:\n fewer returned than expected") +
  scale_x_discrete(name = "", guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(name = "Error: observed - forecast", labels = scales::comma) + #limits drops point error when outside ymax
  scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
  theme(legend.position = "top")

ggsave("O:/code/coho/forecast_wb/f_oat_l3_error.png", width = 7, height = 5)


# ggdata <- bind_rows(oat_obs, rtrn_trail_mean) |> 
#   filter(StockID == 161, str_detect(lag_mod, "l3|l2")) |> 
#   arrange(year, lag_mod) |> 
#   mutate(
#     year = as.character(year),
#     lag = str_sub(lag_mod, 1,2),
#     mod = str_sub(lag_mod, 4, 40))
# 
# ggplot(ggdata, aes(year)) + 
#   geom_col(aes(y = err, color = mod, fill = mod), position = position_dodge(), width = 0.7) +
#   geom_hline(
#     data = ggdata |> group_by(lag_mod, lag, mod) |> summarise(err = mean(err), .groups = "drop"),
#     aes(yintercept = err, color = mod),
#     linetype = "dashed", size = 1.1) +
#   scale_x_discrete(name = "", guide = guide_axis(n.dodge = 2)) +
#   scale_y_continuous(name = "Error, 50th - obs", labels = scales::comma) + #limits drops point error when outside ymax
#   scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
#   facet_wrap(~lag, scales = "free") +
#   theme(legend.position = "top")

```

### Overlaid posterior medians by lag

```{r oat_lag_overlay, fig.width=7, fig.height=7}
oat_lag_overlay <- function(stk = 161){
  d <- oat_obs |> filter(StockID == stk, !str_detect(lag_mod, "pre_")) |> 
    mutate(
      lag = paste("Lag", str_sub(lag_mod, 2,2)),
      mod = toupper(str_sub(lag_mod, 4, 40))
    )
 
  ggplot(d, aes(year)) + 
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_point(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_line(aes(y = `50%`, color = lag), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2), minor_breaks = 1998:2020) +
    scale_y_continuous(name = "Return", labels = scales::comma) + 
    scale_color_manual("", values = as.vector(wacolors::wa_pal("rainier", which = c("ground","winter_sky","paintbrush"))), aesthetics = c("color", "fill")) +
    facet_wrap(~mod, ncol = 1) + 
    theme(legend.position = "top") +
    labs(title = d$pop[1], color = "", fill = "")
  
}

oat_lag_overlay(161)

#ggsave("O:/code/coho/forecast_wb/f_oat_pred_lag_overlay.png", width = 8, height = 9)

oat_lag_overlay(131) + #Quill Fall
oat_lag_overlay(153) +  #Hump
oat_lag_overlay(149) + #Chehalis
oat_lag_overlay(157) + #GH Misc
oat_lag_overlay(63) + #Deschutes
oat_lag_overlay(69) + #Nisq
patchwork::plot_layout(ncol = 2)
```

### Summarized performance measures

```{r gt_perf_measures}
summary_start_year <- 2009

oat_obs |>
  filter(between(year, summary_start_year, 2019)) |> 
  arrange(year, lag_mod) |>
  group_by(StockID, pop, lag_mod) |> 
  summarise(
    msa = 100*(exp(median(abs(lar))) - 1),
    mase = 100*mean(ase),
    # mape = 100*mean(err_abs_pct),
    # rmse = sqrt(mean(err^2)),
    # # me = mean(err),
    # # mpe = 100*median(err_pct),
    # # nse_mod = 1 - ( sum(err_abs) / sum(abs(rtrn - mean(rtrn))) ),
    # # nse = 1 - ( sum(err^2) / sum((rtrn - mean(rtrn))^2) ),
    .groups = "drop") |> 
  mutate(
    lag = str_sub(lag_mod, 2,2),
    lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
    mod = toupper(str_sub(lag_mod, 4, 40)),
    mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) |> 
  filter(StockID == 161) |>
  gt::gt(rowname_col = "mod", groupname_col = "lag") |> 
  gt::cols_hide(c(StockID, pop, lag_mod)) |> 
  gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |> 
  # gt::fmt_percent(columns = c(msa, mase, mape), scale_values = F, decimals = 0) |> 
  # gt::fmt_number(columns = c(rmse), decimals = 0) |> 
  gt::data_color(
    columns = c(msa, mase), 
    colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
  ) |> 
  gt::tab_header(
    title = "One-ahead Performance Measures", 
    subtitle = paste(summary_start_year, "- 2019; AR1 and STIPM calculated from posterior median")) |> 
  gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
  gt::tab_source_note("Mean Abs. Scaled Error (MASE)") |> 
  # gt::tab_source_note("Mean Abs. Percent Error (MAPE)") |> 
  # gt::tab_source_note("Root Mean Square Error (RMSE)") |> 
  # #gt::tab_source_note("Mean Error (ME)")

gt::gtsave("O:/code/coho/forecast_wb/gt_oat_perf_measures_09_19.png", expand = 20)


perf_smry <- function(stk, summary_start_year = 2009){
  gt_data <- oat_obs |>
    filter(StockID == stk, between(year, summary_start_year, 2019)) |> 
    arrange(year, lag_mod) |>
    group_by(StockID, pop, lag_mod) |> 
    summarise(
      msa = 100*(exp(median(abs(lar))) - 1),
      mase = 100*mean(ase),
      .groups = "drop") |> 
    mutate(
      lag = str_sub(lag_mod, 2,2),
      lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
      mod = toupper(str_sub(lag_mod, 4, 40)),
      mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) 
  
   gt_data |> 
    gt::gt(rowname_col = "mod", groupname_col = "lag") |>
    gt::cols_hide(c(StockID, pop, lag_mod)) |>
    gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |>
    gt::data_color(
      columns = c(msa, mase),
      colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
    ) |>
    gt::tab_header(
      title = paste("One-ahead Performance:", gt_data[1,"pop"], gt_data[1,"StockID"]),
      subtitle = paste(summary_start_year, "- 2019; AR1 and STIPM calculated from posterior median")) |>
    gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
    gt::tab_source_note("Mean Abs. Scaled Error (MASE)")
}

perf_smry(161)

# coho_data_tbl |> distinct(StockID) |> pluck("StockID") |> 
#   walk(~perf_smry(.x) |> gt::gtsave(paste("O:/code/coho/forecast_wb/gt_oat_summary/gt_oat_perf_measures_stk",.x, ".png"), expand = 20))



oat_obs |>
  filter(year == 2019) |> 
  arrange(year, lag_mod) |> 
  group_by(StockID, pop, lag_mod) |> 
  summarise(
    msa = 100*(exp(median(abs(lar))) - 1),
    mase = 100*mean(ase),
    # mape = 100*mean(err_abs_pct),
    # rmse = sqrt(mean(err^2)),
    # # me = mean(err),
    # # mpe = 100*median(err_pct),
    # # nse_mod = 1 - ( sum(err_abs) / sum(abs(rtrn - mean(rtrn))) ),
    # # nse = 1 - ( sum(err^2) / sum((rtrn - mean(rtrn))^2) ),
    .groups = "drop") |> 
  mutate(
    lag = str_sub(lag_mod, 2,2),
    lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
    mod = toupper(str_sub(lag_mod, 4, 40)),
    mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) |> 
  filter(StockID == 161) |>
  gt::gt(rowname_col = "mod", groupname_col = "lag") |> 
  gt::cols_hide(c(StockID, pop, lag_mod)) |> 
  gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |> 
  gt::data_color(
    columns = c(msa, mase), 
    colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
  ) |> 
  gt::tab_header(
    title = "One-ahead Performance Measures", 
    subtitle = "2019 only; AR1 and STIPM calculated from posterior median") |> 
  gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
  gt::tab_source_note("Mean Abs. Scaled Error (MAPE)")

  
  


# #some series, e.g., Quillayute Fall (131), are much more accurately forecast...
# #looks like some are just screwy
# oat_obs_msa |> 
#   filter(str_detect(lag_mod, "l3|l2")) |> 
#   pivot_wider(names_from = lag_mod, values_from = msa) |> 
#   arrange(l3_stipm) |> #print(n=40) 
#   gt::gt() |> 
#   gt::fmt_number(columns = -c(StockID, pop), decimals = 1) |> 
#   gt::grand_summary_rows(columns = -c(StockID, pop), fns = list(median = "median", mean = "mean")) |> 
#   gt::data_color(
#     columns = -c(StockID, pop), 
#     colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,400), na.color = "red")
#   ) |> 
#   gt::tab_header("One-ahea Median Symmetric Accuracy (MSA)", subtitle = "Posterior median for AR1 and STIPM predictions")

# #older summaries across other measures
# oat_obs |> 
#   group_by(StockID, pop_id, pop, lag_mod) |> 
#   summarise(across(err:mase, list(mean = ~mean(.), median = ~median(.))), .groups = "drop") |> 
#   select(StockID:lag_mod, contains("median"), contains("mean")) |> 
#   filter(StockID == 161)
  
```


# Full dataset

```{r full_data_fits, eval=FALSE}
#use all data
(
  stan_data_list <- stan_data_filter(
    #coho_data_tbl,
    coho_data_tbl_2020,
    data_year_min = 1998, data_year_max = 2021, 
    lag_smolt = 0, lag_MS = 0, lag_spwn = 0, lag_hvst = 0)
)

# #can construct estimates beyond desired forecast year by "tricking" the wrapper that sets n_year
# #then retain fit objects and extract prediction as adult_est (for AR1) and adult_est+harvest_est (for STIPM)
# stan_data_list$y_min_max <- 1998:2021

print(stan_data_list$y_min_max)
print(c("smolt", range(stan_data_list$smolt$year)))
print(c("MS", range(stan_data_list$MS$year)))
print(c("spwn", range(stan_data_list$spwn$year)))
print(c("hvst", range(stan_data_list$hvst$year)))
print(c("rtrn", range(stan_data_list$rtrn$year)))

fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = 200, n_chain = 2)
fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = 200, n_chain = 2)

# # #on AWS
# fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = 4000, n_chain = 4)
# saveRDS(fit_stipm, "fit_stipm.rds")
# 
# fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = 4000, n_chain = 4)
# saveRDS(fit_ar1, "fit_ar1.rds")

```

```{r read_full_data_fits, eval=FALSE}
# #these are huge, balloons memory
# #per year have rows = 4chains * post-warmup
# 
# fit_ar1 <- readRDS("O:/code/coho/forecast_wb/full_data_fit_ar1.rds")
# 
fit_ar1_wb <- spread_draws(fit_ar1, adult_est[n_year,n_pop] | n_pop) |>
  ungroup() |> select(n_year, .iteration, rtrn = `34`) |>
  mutate(year = first(stan_data_list$y_min_max)-1+n_year, mod = "ar1")
# 
# saveRDS(fit_ar1_wb, "fit_ar1_wb_adult_est.rds")
# 
# fit_stipm <- readRDS("O:/code/coho/forecast_wb/full_data_fit_stipm.rds")
# #stipm requires constructing total return from harvest + spawning adults
fit_stipm_wb <- full_join(
  spread_draws(fit_stipm, adult_est[n_year,n_pop] | n_pop) |>
    ungroup() |> select(n_year, .chain, .iteration, .draw, spwn = `34`)
  ,
  spread_draws(fit_stipm, harvest_est[n_year,n_pop] | n_pop) |>
    ungroup() |> select(n_year, .chain, .iteration, .draw, hvst = `34`)
  ,
  by = c("n_year", ".chain", ".iteration", ".draw")) |>
  mutate(year = first(stan_data_list$y_min_max)-1+n_year, rtrn = spwn+hvst, mod = "stipm") |> 
  bind_rows(
    spread_draws(fit_stipm, adult_pred[n_pop] | n_pop) |> ungroup() |> 
      select(.chain, .iteration, .draw, rtrn = `34`) |> 
      mutate(year = last(stan_data_list$y_min_max)+1, mod = "stipm")
  )
# 
# saveRDS(fit_stipm_wb, "fit_stipm_wb_adult_est.rds")


#can also run fit_stipm_pred and fit_ar1_pred lines from oat chunk
#brings n_eff, Rhat, n_diverg
fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
    mod = "ar1",
    n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")
#recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

fit_ar1_pred |> 
  filter(between(StockID, 130, 170)) |> 
  print(n=40)
fit_stipm_pred |> 
  filter(between(StockID, 130, 170)) |> 
  print(n=40)

bind_rows(fit_ar1_pred, fit_stipm_pred) |> 
  filter(between(StockID, 130, 170)) |> 
  select(year, mod, StockID, StockLongName, pop_id, `2.5%`,`25%`,`50%`,`75%`,`97.5%`) |> 
  pivot_wider(names_from = mod, values_from = `2.5%`:`97.5%`) |> 
  select(year:pop_id, contains("50%"))


```

```{r}
fit_ar1_wb <- readRDS( "fit_ar1_wb_adult_est.rds")
fit_stipm_wb <- readRDS("fit_stipm_wb_adult_est.rds")

est_wb_rtrn <- bind_rows(
  fit_ar1_wb |> select(mod, year, rtrn), 
  fit_stipm_wb |> select(mod, year, rtrn)
  )

est_wb_rtrn_q <- est_wb_rtrn |> 
  group_by(mod, year) |> 
  summarise(across(rtrn, list(
    `2.5` = ~quantile(.x, 0.025),
    `25` = ~quantile(.x, 0.25),
    `50` = ~quantile(.x, 0.5),
    `75` = ~quantile(.x, 0.75),
    `97.5` = ~quantile(.x, 0.975)
    ), .names = "{.fn}"),
    .groups = "drop")


est_wb_rtrn |> 
  filter(between(year, 1998, 2022)) |> 
  ggplot(aes(year, rtrn)) +
  stat_lineribbon(aes(fill = mod), alpha = 0.4, show.legend = F) +
  geom_point(
    #data = coho_data_tbl |> filter(StockID == 161, between(year, 1998, 2022)),
    data = stan_data_list$rtrn |> filter(StockID==161),
    color = 1
  ) +
  facet_wrap(~mod) +
  geom_text(
    data = est_wb_rtrn_q |> filter(year == 2022) |> select(mod, year, rtrn = `50`),
    aes(label = scales::comma(round(rtrn))),
    nudge_x = 1.5
  ) +
  scale_fill_discrete(type = pal_lag_mod[1:2]) +
  scale_x_continuous("", breaks = 1998:2022, minor_breaks = 1998:2022, guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous("Forecast return", labels = scales::comma) +
  labs(title = "Willapa Bay preliminary 2022 forecast", 
       subtitle = "Posterior intervals of fits to 1998-2020 data",
       fill = "Model") +
  theme_light(base_size = 13)

##ggsave("O:/code/coho/forecast_wb/f_example_2022.png", width = 11, height = 7)


```

```{r}
pdrw <- bind_rows(
  #stipm
  #pre-fcst year estimate series
  full_join(
    spread_draws(fit_stipm, adult_est[n_year,n_pop] | n_pop) |> ungroup() |> 
      pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "spwn"),
    spread_draws(fit_stipm, harvest_est[n_year,n_pop] | n_pop) |> ungroup() |> 
      pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "hvst"),
    by = c("n_year", ".chain", ".iteration", ".draw", "pop_id")
  ) |>
    mutate(
      year = first(stan_data_list$y_min_max)-1+n_year, n_year = NULL,
      rtrn = spwn+hvst,
      mod = "stipm") |> 
    #add fcst year
    bind_rows(
      spread_draws(fit_stipm, adult_pred[n_pop] | n_pop) |> ungroup() |> 
        pivot_longer(cols = -c(.chain, .iteration, .draw), names_to = "pop_id", values_to = "rtrn") |> 
        mutate(year = last(stan_data_list$y_min_max)+1, mod = "stipm")
    )
  ,
  #ar1
  spread_draws(fit_ar1, adult_est[n_year,n_pop] | n_pop) |> ungroup() |>
    pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "rtrn") |> 
    mutate(
      year = first(stan_data_list$y_min_max)-1+n_year, n_year = NULL,
      mod = "ar1")  
  ) |> 
  mutate(pop_id = as.numeric(pop_id)) |> 
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

```

```{r}
ggfcst_ts <- function(stk = 161) {
  pdrw_stk <- filter(pdrw, StockID == stk, between(year, 1998, 2022))
  
  # #could add logic for function arg ar1only = T 
  # pdrw_stk <- filter(pdrw_stk, mod == "ar1") 
  
  ggplot(pdrw_stk, aes(year, rtrn)) +
    stat_lineribbon(aes(fill = mod), alpha = 0.4, show.legend = F) +
    geom_point(data = filter(stan_data_list$rtrn, StockID==stk), color = 1) +
    facet_wrap(~mod) +
    scale_fill_discrete(type = pal_lag_mod[1:2]) +
    #scale_x_continuous("", breaks = 1998:2022,  guide = guide_axis(n.dodge = 2)) +
    scale_x_continuous("", breaks = seq(1998, 2022, by = 2),  guide = guide_axis(angle = 45)) +
    scale_y_continuous("Forecast return", labels = scales::comma) +
    labs(title = paste(pdrw_stk$StockLongName[1], "Preliminary 2022 forecast"), 
         subtitle = "Posterior intervals of fits to 1998-2020 data",
         fill = "Model") +
    theme_light(base_size = 13)
}

ggfcst_ts()
```

```{r}
ggfcst_ts_coast <- map(
  coho_data_tbl |> distinct(StockID, StockLongName) |> filter(StockID > 120) |> pluck("StockID") |> set_names(), 
 ~ggfcst_ts(.x))

patchwork::wrap_plots(ggfcst_ts_coast)
```

