---
title: "2023 Willapa Bay natural coho forecast"
author: "" 
date: "`r Sys.Date()`"
params:
  f_pop_stock_metadata: "data/pop_stock_metadata.csv"
  f_prior_year_dataset: "data/fram_cwt_smolt_fulljoin_for2022.csv"
  f_fram_mdb: "~/O/code/coho/fram_mdbs/PSC_CoTC_PostSeason_CohoFRAMDB_thru2021_021523.mdb"
  f_fram_prelim_escp: "~/T/DFW-Salmon Mgmt Modeling Team - General/Escapement files/Coho/fram_coho_escapement_2023.xlsx"
  f_smolt_migrants: "data/Smolt Time Series.xlsx"
  f_cwt_rel_rec_lookup: "data/rmis_survival_hatchery_release_location.csv"
  f_cwt_releases: "data/rmis_releases_all_coho_by2010_onward.csv"
  f_cwt_recoveries: "data/rmis_recs_coho_2019_2022.csv"
editor_options: 
  chunk_output_type: console
output: 
  bookdown::html_document2
---

# Summary

This script presents preseason forecasts for Willapa Bay (WB) natural coho, generated by the Washington Department of Fish and Wildlife (WDFW). The underlying datasets depend on the valuable contributions of many technical staff from Washington's salmon comanagers. The methods were originally described for a [2021 SSC review](https://www.pcouncil.org/documents/2021/10/f-1-attachment-3-a-proposed-forecast-methodology-for-natural-origin-willapa-bay-coho-o-kisutch-electronic-only.pdf/), and are grounded in [DeFilippo et al.'s 2021 publication](https://www.sciencedirect.com/science/article/pii/S0165783621001429) "Improving short-term recruitment forecasts for coho salmon using a spatiotemporal integrated population model". The quantified uncertainty in this approach supports fishery managers' evaluation of the relative risks of alternatives.

# Setup

The following code readies the R environment (`r sessionInfo()$R.version$version.string`), calling necessary packages and loading an initial dataset.

```{r setup, results = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, warning = FALSE, message = FALSE, fig.width = 9, fig.height = 9)

library("odbc"); library("DBI")

library("tidyverse")
library("gt")
library("rstan")
options(mc.cores = 4)
rstan_options(auto_write = TRUE)
theme_set(theme_light())
library("tidybayes")

pop_stock_metadata <- readr::read_csv(params$f_pop_stock_metadata)
pop_stock_metadata_sf <- pop_stock_metadata |>
  sf::st_as_sf(
    coords = c("lon", 'lat'),
    crs = sf::st_crs("+proj=longlat +datum=WGS84")
    ) |>
  sf::st_transform(crs = sf::st_crs(32610))

#ar1, stipm, trailing arith mean, official preseason forecast
pal_lag_mod <- c("purple", "gold", "cyan", "pink") 

# see below for update integration:
# - first as 'prelim' with smolts, CWTs, escp and lm(log(hvst) ~ log(spwn))
# - then redone after CoTC postseason mdb available
#coho_data_tbl_prelim <- read_csv("data/fram_cwt_smolt_fulljoin_for2023_prelim.csv")
coho_data_tbl <- read_csv("data/fram_cwt_smolt_fulljoin_for2023.csv")


# pdrw <- readRDS("O:/code/coho/forecast_wb/pdrw_2020.rds")
# pdrw <- readRDS("~/O/code/coho/forecast_wb/pdrw_2020.rds")
# pdrw |> filter(StockID==161, year > 2019) |> group_by(mod, StockID, year) |> summarise(across(c(spwn, hvst, rtrn), ~median(.,na.rm=T)))

```

# Data

The analysis relies on 
  - Per-stock FRAM postseason estimates of spawning escapement and harvest mortality, 
  - RMIS CWT releases and recoveries to estimate marine survival, and 
  - Smolt outmigrant estimates compiled by tribal comanagers and WDFW.

See work in [https://github.com/wdfw-fp/coho_STIPM/tree/main/older_scripts](https://github.com/wdfw-fp/coho_STIPM/tree/main/older_scripts) for full development sequence of original datasets.

Coho FRAM tracks age 3 fish across 5 time steps corresponding to a calendar year. It includes unmarked and marked units of both natural and hatchery stocks. Data in the best available database, maintained by PSC CoTC, extend to 1986, but values prior to 1998 are of unknown origin and those from 2010-onward have received the most review and QAQC.

A lookup provided by Ty Garber (WDFW) maps RMIS hatchery and release location fields to the stocks/populations in the analysis. This is used against RMIS queries to derive the releases and recoveries that form the basis for marine survival estimation. These values could be revisited "from scratch" with comprehensive query files, but here showing only a recent year update. 

Smolt trap estimates are included from a dataset maintained by Marisa Litz (WDFW). Updated data are manually prepared in a workbook with a sheet organized around the relevant FRAM units for the analysis.

The data compilation below includes a chunk to rebuild the FRAM values from a post-season run database (thereby accommodating changes to past years as well as the annual addition of another post-season year), as well as an update to the RMIS CWT values (here for two additional years only, but allowing updates to n-many-years given more comprehensive release and recovery query files). 

During a new cycle of annual forecast production, the `dplyr::rows_upsert` function then adds new observations to the prior "full dataset" and replaces older ones that have changed. 

```{r rebuild_fram_from_mdb, eval=FALSE}
# If rebuilding from post-season run database
## TAMMS at "DFW-Salmon Mgmt Modeling Team - General/FRAM Validation Runs/Coho/"

fram_mdb_reader <- function(
    mdb_file_path = params$f_fram_mdb,
    fram_table,
    osx = grepl("apple", sessionInfo()$platform)
    ) {
  if(osx){
    t_out <- readr::read_csv(I(
      system2(
        "mdb-export",
        args = paste(str_replace_all(mdb_file_path, " ", "\\\\ "),
                     fram_table),
        stdout = T) 
    ))
  } else {
    db_con <- DBI::dbConnect(
      drv = odbc::odbc(),
      .connection_string = paste0(
        "Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=", mdb_file_path, ";")
      )

    t_out <- DBI::dbReadTable(db_con, fram_table)

    DBI::dbDisconnect(db_con)
  }
  return(t_out)
}

fram_stocks <- c(105,93,89,45,51,55,75,81,61,23,149,63,107,115,111,157,97,135,153,101,69,1,85,139,131,127,145,11,17,59,35,29,117,161)
fram_runs <- 22:45 #1998 forward, or 34 start for 2010 onward
fram <- list()

#Note the original negative escapement in A12A Wild in 2005 is due to an error in the FRAM database
#here, spwn is coerced to 0, but harvest is unaffected b/c drawn directly from Mortality
#note also inserting NAs for Grays Hbr Misc Wild missing 2000 & 2001 in FRAM mdb (Area 13A Miscellaneous Wild missing 1996, but now 1998-forward)
### not preserving fishery-specific mort, since aggregating to per-year-per-stock
## fram$fishery <- fram_mdb_reader(fram_table = "Fishery") |> filter(Species == "COHO") |> select(FisheryID, FisheryName)
## fram$stock <- fram_mdb_reader(fram_table = "Stock") |> filter(Species == "COHO", StockID %in% fram_stocks) |> select(StockID, StockLongName)
fram$runid <- fram_mdb_reader(fram_table = "RunID") |> 
  filter(RunID %in% fram_runs) |> 
  select(RunID, year = RunYear)
fram$escp <- fram_mdb_reader(fram_table = "Escapement") |> 
  filter(StockID %in% fram_stocks, RunID %in% fram_runs) |> 
  select(RunID, StockID, spwn = Escapement) |> 
  mutate(spwn = if_else(spwn < 0, 0.01, spwn))
fram$mort <- fram_mdb_reader(fram_table = "Mortality") |> 
  filter(StockID %in% fram_stocks, RunID %in% fram_runs) |> 
  mutate(
    mort = LandedCatch+NonRetention+Shaker+DropOff + MSFLandedCatch+MSFNonRetention+MSFShaker+MSFDropOff
    ) |> 
  group_by(RunID, StockID) |> 
  summarise(hvst = sum(mort), .groups = "drop")

#unpiped for clarity
fram$spwn_hvst_rtrn <- full_join(fram$escp, fram$mort, by = c("RunID", "StockID")) |> 
  bind_rows(tibble(RunID = c(24, 25), StockID = c(157, 157)))
fram$spwn_hvst_rtrn <- full_join(fram$runid, fram$spwn_hvst_rtrn, by = "RunID") |> select(-RunID)
fram$spwn_hvst_rtrn <- full_join(pop_stock_metadata, fram$spwn_hvst_rtrn, by = "StockID")
fram$spwn_hvst_rtrn <- fram$spwn_hvst_rtrn |> mutate(rtrn = spwn + hvst)

```

```{r rmis_updates, eval=FALSE}
#pop_stock_metadata |> arrange(StockID) |> drop_na(smolt_ocn_surv_pop) |> pull(smolt_ocn_surv_pop) |> paste(collapse = "', '")
ocn_surv_pops <- c('Nooksack H', 'Skagit H', 'Baker W', 'Big Beef Crk W', 'Quilcene H', 'Skokomish H', 'Deschutes W', 'Kalama Crk H', 'Minter Crk W', 'Puyallup H', 'Green H', 'Dungeness H', 'Quinault H', 'Bingham Crk W', 'Willapa H')

rmis <- list()
rmis$lu <- read_csv(params$f_cwt_rel_rec_lookup) |> 
  mutate(watershed = str_replace(watershed, "Creek", "Crk")) |> 
  unite("smolt_ocn_surv_pop", watershed, rearing_type, sep = " ", remove = F) |> 
  filter(smolt_ocn_surv_pop %in% ocn_surv_pops)
rmis$rel <- read_csv(params$f_cwt_releases)
rmis$rec  <- read_csv(params$f_cwt_recoveries) |> 
  drop_na(estimated_number) |> 
  mutate(
    rec_year = lubridate::year(as.Date(as.character(recovery_date), format = "%Y%m%d")),
    rec_year = if_else(is.na(rec_year) & nchar(as.character(recovery_date)) == 4,
                       as.integer(recovery_date), as.integer(rec_year)),
    rec_year = if_else(is.na(rec_year) & nchar(as.character(recovery_date)) == 6,
                       as.integer(str_sub(as.character(recovery_date),1,4)), as.integer(rec_year)),
    rec_age = rec_year - brood_year
  )

rmis$est_n_rel_rec <- rmis$lu |> 
  inner_join(
    rmis$rel, 
    by = c("species", "hatchery_location_code", "release_location_code", "rearing_type", "run")
    ) |> 
  select(
    smolt_ocn_surv_pop, brood_year,
    tag_code = tag_code_or_release_id, 
    release_scalar, tagged_adclipped:untagged_unknown) |>
  mutate(est_n_rel = (tagged_adclipped + tagged_unclipped) * release_scalar) |>
  arrange(brood_year) |> 
  select(-contains("agged")) |> 
  filter(est_n_rel > 0) |> 
  left_join(
    rmis$rec |> 
      group_by(tag_code) |> 
      summarise(est_n_rec = sum(estimated_number), .groups = "drop")
    ,
    by = c("tag_code")
  ) |>
  #aggregate across tag_code(s) 
  group_by(smolt_ocn_surv_pop, brood_year) |> 
  summarise(across(c(est_n_rel, est_n_rec), ~sum(., na.rm = T)), .groups = "drop") |> 
  #"Calendar Year" in orig 
  mutate(year = brood_year + 2) 

# #recoveries not currently constrained by age/year-relative-to-broodyear
# #that is, including all recoveries per tag_code
# #but need to define relationship between between release and recovery files
# #relative to each other and to prior dataset
# #here, the original dataset from Oct 2021 contained values through recovery year 2018
# coho_data_tbl_prior |> drop_na(est_n_rec) |> pull(year) |> max()
# #the release update file has groups from brood year 2010-2021
# range(rmis$rel$brood_year)
# #and the recoveries file was queried in late 2022 for recovery years 2019 onward
# #likely contains some errors for brood- and/or recovery-year
# #but is adequately complete for updating brood_years 2017 and 2018 ('year' 2019 and 2020)
# rmis$rec |> count(brood_year, rec_year)


# ## quick look at questionable recovery records
# rmis$rec |> 
#   group_by(brood_year, rec_year, tag_code) |> 
#   summarise(est_n_rec = sum(estimated_number), .groups = "drop") |>
#   mutate(diff_year = rec_year - brood_year) |> 
#   arrange(desc(diff_year)) |> filter(diff_year > 3) |> print(n=100)

# #QAQC to illustrate 
# # - matches for (rec)'year' 2017-18 (brood 15-16)
# # - adequate records for (rec)'year' 2019-20 (brood 17-18)
# # - inadequate recs for (rec)'year' 2021 (brood 19)
# rmis$est_n_rel_rec |> 
#   filter(between(year, 2017, 2021)) |> 
#   left_join(
#     coho_data_tbl_prior |> 
#       drop_na(est_n_rec, est_n_rel) |> 
#       distinct(StockID, StockLongName, 
#                smolt_ocn_surv_pop, year,
#                est_n_rel_orig = est_n_rel,
#                est_n_rec_orig = est_n_rec)
#     , by = c("smolt_ocn_surv_pop", "year")
#   ) |> 
#   mutate(
#     d_rel = round(est_n_rel - est_n_rel_orig),
#     d_rec = round(est_n_rec - est_n_rec_orig)
#   ) |> 
#   print(n=100)

rmis$est_n_rel_rec <- rmis$est_n_rel_rec |> 
  filter(between(year, 2019, 2020)) |> 
  select(-brood_year)

```

```{r coho_data_tbl, eval=FALSE}
coho_data_tbl_prior <- read_csv(params$f_prior_year_dataset)

# anti_join(select(coho_data_tbl, StockID, year), select(coho_data_tbl_prior, StockID, year), by = c("StockID", "year"))
# anti_join(select(coho_data_tbl_prior, StockID, year), select(coho_data_tbl, StockID, year), by = c("StockID", "year")) |> count(year)

#first update any FRAM values that have changed
#then append RMIS CWT and smolt outmigrant updates
coho_data_tbl <- coho_data_tbl_prior |> 
  filter(year >= 1998) |> 
  rows_upsert(
    fram$spwn_hvst_rtrn, #generated above from FRAM mdb
    by = c("pop_id", "pop", "StockID", "StockLongName", 
           "lon","lat","hab_km","smolt_ocn_surv_pop","hat",
           "year")
  ) |> 
  rows_upsert(
    rmis$est_n_rel_rec,
    by = c("smolt_ocn_surv_pop", "year")
  ) |> 
  rows_upsert(
    #pre-add metadata to avoid NAs for stock-years not yet in dataset
    #Snohomish new from 2002-onward, otherwise handful of new 2022 vals
    right_join(
      pop_stock_metadata,
      readxl::read_excel(params$f_smolt_migrants, sheet = "stipm_2023") |> 
        filter(year >= 1998) |> 
        select(year, StockID, smolt),
      by = c("StockID"))
    ,
    by = c("pop_id", "pop", "StockID", "StockLongName", 
           "lon","lat","hab_km","smolt_ocn_surv_pop","hat",
           "year")
  )

```

Currently, the most recent complete post-season run becomes available during February of the annual preseason process, typically just before final forecasts are needed.

Compiled escapement estimates and simple regression-based estimates of stock specific catch can be added in order to produce preliminary forecasts before this database becomes available.

Further preliminary escapement estimates for the most recent year can also be added if available, with harvest following the same approach as for a not-yet-available FRAM post-season run.

```{r coho_data_tbl_prelim, eval=FALSE}
coho_data_tbl_prelim <- coho_data_tbl |> 
  rows_upsert(
    left_join(
      pop_stock_metadata,
      readxl::read_excel(
        params$f_fram_prelim_escp,
        range = "BKFRAM!A4:Z168", col_types = c("numeric", "text", rep("numeric", length(1998:2021)))) |>
        select(StockID = `Row Labels`, spwn = `2021`) |>
        mutate(year = 2021)
      , by = c("StockID")
    )
    ,
    by = c("pop_id", "pop", "StockID", "StockLongName", 
           "lon","lat","hab_km","smolt_ocn_surv_pop","hat",
           "year")
  )


# need hvst and rtrn to run prelim AR1
# this adds a log-linear guess of 2021 harvest for stocks with r^2 > 0.4 in 2016-2020
# otherwise just the 2016-2020 mean
coho_data_tbl_prelim <- rows_upsert(
  coho_data_tbl_prelim
  ,
  coho_data_tbl_prelim |>
    select(StockID, year, spwn, hvst) |> 
    filter(between(year, 2016, 2020)) |>
    nest(data = -c(StockID)) |> 
    #fit simple log-linear for decent correlation stocks
    mutate(lm = map(data, ~lm(log(hvst) ~ log(spwn), data = .x))) |>
    #add new year of escapement to predict on
    left_join(
      coho_data_tbl_prelim |> 
        filter(year == 2021) |> 
        select(StockID, StockLongName, spwn),
      by = c("StockID")
    ) |>
    nest(newdata = spwn) |>
    mutate(
      r2 = map_dbl(lm, ~summary(.x) |> pluck("r.squared")),
      hvst = map2_dbl(lm, newdata, ~predict.lm(.x, newdata = .y) |> exp()),
      hvst_mean = map_dbl(data, ~.x$hvst |> mean(na.rm=T))
    ) |>
    unnest(newdata) |> 
    #arrange(desc(r2)) |> print(n = 50)
    mutate(
      year = 2021,
      hvst = if_else(r2 > 0.4, hvst, hvst_mean),
      rtrn = spwn + hvst
      ) |>
    drop_na(spwn) |> 
    select(StockID, year, spwn, hvst, rtrn)
  ,
  by = c("StockID", "year"))

coho_data_tbl_prelim |> filter(is.na(pop_id))
coho_data_tbl_prelim |> filter(is.na(rtrn)) |> print(n=50)
coho_data_tbl_prelim |> filter(year==2021) |> print(n=50)
```

```{r coho_data_tbl_add_WB_most_recent_write_csv, eval=FALSE}
# WB 2022 prelims:
#  nat UM: 29263 NOS + 423 NOB = spwn = 29686
#  (hat M: 10484 HOS + 59019 HOB)

# coho_data_tbl_prelim <- rows_upsert(
#   coho_data_tbl_prelim
coho_data_tbl <- rows_upsert(
  coho_data_tbl
  ,
  right_join(
    pop_stock_metadata,
    tibble(
      StockID = 161, year = 2022, spwn = 29686,
      hvst = lm(
        log(hvst) ~ log(spwn), 
        data = filter(coho_data_tbl_prelim, StockID==161, between(year, 2016, 2020))
      ) |> 
        predict(tibble(spwn = spwn)) |> exp(),
      rtrn = spwn + hvst
    ),
    by = "StockID"
  )
  ,
  by = c("pop_id", "pop", "StockID", "StockLongName", 
         "lon","lat","hab_km","smolt_ocn_surv_pop","hat",
         "year")
)

coho_data_tbl_prelim |> filter(year==2022) |> print(n=50)
coho_data_tbl |> filter(year==2022) |> print(n=50)

#write_csv(coho_data_tbl_prelim, "data/fram_cwt_smolt_fulljoin_for2023_prelim.csv")
write_csv(coho_data_tbl, "data/fram_cwt_smolt_fulljoin_for2023.csv")

```


# Model fitting

The one-ahead forecast skill evaluation that was demonstrated in the Oct 2021 review is expected to be performed for the 2023 effort, but it was not re-run during the 2022 exercise due to the limited dataset changes and the minor performance differences between the 2 current model configurations.

## `stan` functions

Forecast skill evaluation requires re-fitting models to progressive subsets of the data filtered by year. Rather than a "sliding window" (e.g., the prior 10 years stepped forward each year), the dataset is "stretched" from a fixed starting year. Here, a helper function trims years from the complete dataset according to arguments controlling the data-type-specific lag from a data `data_year_max` corresponding to the year before the desired forecast year (i.e., a `data_year_max` of 2021 designates the most recent possible data for the 2022 forecast). 

```{r stan_data_filter}
#trim years then declare separate intermediaries
#"data_year_max" is "last/max year of available data", so desired year_pred-1
#lags allow for flexible OAT configurations with additional trimming
#but can be zeroed to allow all available data
#for a 2022 forecast in adult_pred vectors from stan
#data_year_max is 2021, s.t. firmly available postseason FRAM is 2019 
stan_data_filter <- function(
  full, #complete dataset
  data_year_min, #starting anchor year
  #in STIPM, adult_pred[data_year_max+1] is a total return from smolt[data_year_max]*surv[data_year_max], with adult_est[data_year_max] as the spawning escapement
  #in AR1, adult_est and adult_pred are total return, and adult_pred[year_pred] is same as last year of adult_est b/c passing n_year+1 relative to STIPM
  data_year_max,
  lag_smolt = 0, #n-extra-years trimmed from smolt abundances 
  lag_MS = 0, #n-extra-years trimmed from release and recovery data
  lag_spwn = 0, #n-extra-years trimmed from FRAM escapement
  lag_hvst = 0 #n-extra-years trimmed from FRAM harvest
  ){
  
  full_ymin_ymax = filter(full, between(year, data_year_min, data_year_max)) |> 
    arrange(pop_id, year) |>  #ensure everything sorted to prevent slice point mishaps
    mutate(yr = year - min(year) + 1)  #reindexing for stan
  
  stan_data = list(
    y_min_max = data_year_min:data_year_max,
    smolt = filter(full_ymin_ymax, !is.na(smolt), year <= data_year_max - lag_smolt),
    MS = filter(full_ymin_ymax, !is.na(est_n_rec), year <= data_year_max - lag_MS),
    spwn = filter(full_ymin_ymax, !is.na(spwn), year <= data_year_max - lag_spwn),
    hvst = filter(full_ymin_ymax, !is.na(hvst), year <= data_year_max - lag_hvst),
    rtrn = filter(full_ymin_ymax, !is.na(rtrn), year <= data_year_max - max(c(lag_spwn, lag_hvst)))
  )
  
  return(stan_data)
}

#stan_data_filter(coho_data_tbl, 2019, 2022) |> map(tail) 
```

Next, wrappers to the `rstan::stan` function facilitate repeated fitting with consistent control parameters and input data lists.

```{r ar1_functions}
stan_ar1 <- function(stan_data, n_iter = 200, n_chain = 2){
  #note following orig naming convention where "tot" refers to "total return" = spwn + hvst = rtrn, as estimated from FRAM
  stan_fit <- stan(
    file = 'stan/LD_coho_forecast_AR_ind_2_v4.stan',
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.999, max_treedepth = 12),
    data = list(
      n_year = length(stan_data$y_min_max) + 1, #sets year of adult_pred and year-dim of adult_est 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      n_pop_tot = length(unique(stan_data$rtrn$pop_id)), #Number of populations with return data
      pop_tot = unique(stan_data$rtrn$pop_id), #Which populations possess return data
      n_tot = length(stan_data$rtrn$rtrn), #Length of the return data vectors
      tot_dat = stan_data$rtrn$rtrn,  #Vectors of all return data across all populations
      tot_true = stan_data$rtrn$yr, #Vectors of the indices identifying which years are those with non-NA data for the return data
      #Paired vectors of slice points indicating the beginning, and end of the data for a particular population
      slice_tot_start = pmatch(unique(stan_data$rtrn$pop_id), stan_data$rtrn$pop_id), #stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_tot_end = c(tail(pmatch(unique(stan_data$rtrn$pop_id), stan_data$rtrn$pop_id)-1, -1), length(stan_data$rtrn$pop_id)) #stan_data$rtrn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  
  return(stan_fit)
}

```

```{r stipm_functions}
stan_stipm <- function(stan_data, n_iter = 200, n_chain = 2){
  #note n_year does not include "+1" of AR1 since adult_pred is calc'd separately as
  # smolt[n_year]*surv[n_year], thereby giving adult run size (spwn+hvst) in n_year+1
  #stan_data$y_min_max accordingly is first:last_year_of_data
  stan_fit <- stan(
    file = 'stan/LD_coho_forecast_6_2_4.stan', 
    iter = n_iter, chains = n_chain, thin = 1, seed = 222,
    control = list(adapt_delta = 0.99, max_treedepth = 10.25),
    data = list(
      n_year = length(stan_data$y_min_max), 
      n_pop = length(unique(coho_data_tbl$pop_id)), #number of populations in full dataset
      u = matrix(1, nrow = 1, ncol = length(unique(stan_data$spwn$pop_id))),
      dist = units::drop_units(sf::st_distance(pop_stock_metadata_sf)/10000), #values are identical
      
      pop_smolt = unique(stan_data$smolt$pop_id), #pop_ids with smolt data 
      n_pop_smolt = length(unique(stan_data$smolt$pop_id)),
      smolt_true = stan_data$smolt$yr,
      smolt_dat = stan_data$smolt$smolt,
      n_smolt = nrow(stan_data$smolt),
      
      pop_esc = unique(stan_data$spwn$pop_id),  # pop_ids with escapement data
      n_pop_esc = length(unique(stan_data$spwn$pop_id)),
      esc_true = stan_data$spwn$yr,
      esc_dat = stan_data$spwn$spwn,
      n_esc = nrow(stan_data$spwn),
      
      pop_catch = unique(stan_data$hvst$pop_id), #pop_ids with harvest data
      n_pop_catch = length(unique(stan_data$hvst$pop_id)),
      harvest_true = stan_data$hvst$yr,
      harvest_dat = stan_data$hvst$hvst,
      n_harvest = nrow(stan_data$hvst),
      
      pop_MS = unique(stan_data$MS$pop_id), #pop_ids with marine survival data
      n_pop_MS = length(unique(stan_data$MS$pop_id)),
      MS_true = stan_data$MS$yr, #orig uses Fishery_Plus_Escapement rather than Release_No to filter...
      MS_dat_x = stan_data$MS$est_n_rec |> round() |> as.integer(), #stan expects integer; previously labeled Fishery_Plus_Escapement
      MS_dat_N = stan_data$MS$est_n_rel |> round() |> as.integer(), #previously labeled Release_No
      n_MS = nrow(stan_data$MS),
      
      stream_dist = stan_data$spwn |> distinct(pop, hab_km) |> pluck("hab_km"),
      
      sigma_esc = 0.2,
      
      n_hatchery = filter(stan_data$MS, hat == 1) |> distinct(pop_id) |> nrow(),
      hatchery = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat > 0, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),
      wild = distinct(stan_data$MS, pop_id, pop, hat) |> 
        mutate(hat_id = if_else(hat < 1, row_number(), NA_integer_)) |> 
        filter(!is.na(hat_id)) |> pluck("hat_id"),

      slice_smolt_start = pmatch(unique(stan_data$smolt$pop_id), stan_data$smolt$pop_id), #stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_smolt_end = c(tail(pmatch(unique(stan_data$smolt$pop_id), stan_data$smolt$pop_id)-1, -1), length(stan_data$smolt$pop_id)), #stan_data$smolt |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_esc_start = pmatch(unique(stan_data$spwn$pop_id), stan_data$spwn$pop_id), #stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_esc_end = c(tail(pmatch(unique(stan_data$spwn$pop_id), stan_data$spwn$pop_id)-1, -1), length(stan_data$spwn$pop_id)), #stan_data$spwn |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_harvest_start = pmatch(unique(stan_data$hvst$pop_id), stan_data$hvst$pop_id), #stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_harvest_end = c(tail(pmatch(unique(stan_data$hvst$pop_id), stan_data$hvst$pop_id)-1, -1), length(stan_data$hvst$pop_id)), #stan_data$hvst |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid"),
      
      slice_MS_start = pmatch(unique(stan_data$MS$pop_id), stan_data$MS$pop_id), #stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = min(rowid), .groups = "drop") |> pluck("rowid"),
      slice_MS_end = c(tail(pmatch(unique(stan_data$MS$pop_id), stan_data$MS$pop_id)-1, -1), length(stan_data$MS$pop_id)) #stan_data$MS |> rowid_to_column() |> group_by(pop_id) |> summarise(rowid = max(rowid), .groups = "drop") |> pluck("rowid")
    )
  )
  return(stan_fit)
}
```

# One-ahead *post-hoc* forecast performance evaluation

This section not rerun for 2023, but revised to a single set of data lags reflecting as-applied experience. The one-at-a-time model fitting proceeds by iterating through a vector of "max data years", predicting the following year by first filtering down a subset of the estimation data, then calling the stan models, and then extracting diagnostic and posterior results. Deterministic trailing means of the OA3 return are also generated for comparison.

```{r oat_wrap, eval=FALSE}
#intermediate per-year saved out within map() in case of loop disruptions
#lag defaults based on 2022 and 2023 forecast application
#to predict 2023, max possible data year is 2022
#using lag_spwn/hvst=1 gives 2021 FRAM
#with lag_smolt=0 for 2022 outmigrants
#and lag_MS=2 gives 2020 RMIS recoveries
#which was latest mostly complete brood

oat_wrap <- function(
    iter, chain, oat_data_year_min, oat_years_max,
    #deducted from data year
    lag_spwn = 1, lag_hvst = 1, lag_MS = 2, lag_smolt = 0
){
  #build a list named by max data year 
  oat <- set_names(oat_years_max) |> 
    map(function(x) {
      print(paste("max data year", x, "predicting", x+1))
      stan_data_list <- stan_data_filter(coho_data_tbl, data_year_min = oat_data_year_min, data_year_max = x, lag_spwn = lag_spwn, lag_hvst = lag_hvst, lag_MS = lag_MS, lag_smolt = lag_smolt) 
      print(stan_data_list$y_min_max)
      print(c("smolt", range(stan_data_list$smolt$year)))
      print(c("MS", range(stan_data_list$MS$year)))
      print(c("spwn", range(stan_data_list$spwn$year)))
      print(c("hvst", range(stan_data_list$hvst$year)))
      print(c("rtrn", range(stan_data_list$rtrn$year)))

      print(paste("AR1 start", Sys.time()))

      fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall AR1 stan data arg n_year+1 relative to STIPM gives adult_pred in desired year
      fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "ar1",
          n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      print(paste("STIPM start", Sys.time()))

      fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = iter, n_chain = chain)
      #recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
      fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
        as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
        mutate(
          var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
          pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
          year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
          mod = "stipm",
          n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
            map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
        ) |>
        left_join(distinct(coho_data_tbl, pop_id, pop, StockID), by = "pop_id")

      pred_out <- bind_rows(fit_ar1_pred, fit_stipm_pred)
      saveRDS(pred_out, 
              paste0("oat_pred_",x+1,"_lags_spwn",lag_spwn,"_hvst",lag_hvst,"_MS",lag_MS,"_smolt",lag_smolt,".rds"))
      return(pred_out) #list element for max-data-year-x 
    })
  return(oat)
}
```

```{r oat_out_run, eval=FALSE}
#run the actual loop, predicting 2009 forward
#can tweak iterations and chains if desired
oat_out <- oat_wrap(
  iter = 1000, chain = 2, 
  oat_data_year_min = 1998, 
  oat_years_max = 2008:2020,
  lag_spwn = 1, lag_hvst = 1, 
  lag_MS = 2, lag_smolt = 0
  )

```

```{r read_bind_oat_fits, eval=FALSE}
#bind up per-year output from the one-ahead runs into a single table
list.files(pattern = "oat_fits", full.names = T) |>
  map_df(~readRDS(.x)) |> 
  select(mod, year, StockID, pop_id, pop, n_diverg, n_eff, Rhat, `2.5%`:`97.5%`) |>
  saveRDS("oat_out_all_years.rds")
```

```{r trailing_means, eval=FALSE}
#create a temp col of the postseason FRAM values available in a given forecast year
#then create rolling mean over prior values
#no longer joining the MB-compiled submitted forecasts
rtrn_trail_mean_fcst <- coho_data_tbl |> 
  select(StockID, pop_id, pop, year, rtrn) |>
  group_by(StockID, pop_id, pop) |> 
  mutate(
    lag2_rtrn = c(NA, NA, head(rtrn, -2)),
    l2_trail_mean = slider::slide_dbl(lag2_rtrn, ~mean(., na.rm=T), .before = 2)
  ) |> 
  ungroup() |> 
  filter(year >= 2009) |> #or other min-data-year in oat_out
  select(-starts_with("lag")) |>  
  pivot_longer(cols = contains("trail_mean"), names_to = "mod", values_to = "50%") 

```

These are bound together and performance measures are calculated.

```{r read_oat_and_oat_obs, eval=FALSE}
oat <- readRDS("oat_out_all_years.rds")

(
oat_obs <- left_join(
  bind_rows(
    oat |> select(mod, year, StockID, pop_id, pop, `2.5%`:`97.5%`),
    rtrn_trail_mean_fcst |> select(-rtrn) #will add back in next join
    ),
  coho_data_tbl |> 
    select(year, StockID, rtrn) |> 
    group_by(StockID) |>  #adding MASE denominator
    mutate(scale_err = mean(abs(rtrn - c(NA, head(rtrn, -1))), na.rm = T)) |> 
    ungroup(),
  by = c("year", "StockID")
  ) |>
  mutate(
    in_50 = (rtrn >= `25%`) & (rtrn <= `75%`),
    in_95 = (rtrn >= `2.5%`) & (rtrn <= `97.5%`),
    #convention: actual - forecast
    #possibly counterintuitive, but
    #negative is overforecast ("fewer returned than predicted"),
    #positive is underforecast ("more returned than predicted")  
    err = rtrn - `50%`,
    err_log = log(rtrn) - log(`50%`),
    err_abs_pct = abs(err / rtrn),
    lar = log(`50%`/rtrn), #log accuracy ratio, used for median symmetric accuracy as described in Morley et al 2018
    ase = abs(err) / scale_err #absolute scaled error for MASE
  ) |> 
  arrange(year, StockID, mod)
)

```

## Diagnostics

```{r oat_rhat_neff_n_diverg, eval=FALSE}
# #all years, all pops
# oat |> 
#   group_by(mod) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop")

# #by year, all pops
# oat |> 
#   group_by(mod, year) |> 
#   summarise(
#     n_diverg = max(n_diverg),
#     Rhat_med = median(Rhat), Rhat_max = max(Rhat),
#     n_eff_med = median(n_eff), n_eff_min = min(n_eff),
#     .groups = "drop") |> 
#   #  print(n = 100)
#   gt::gt(rowname_col = "mod") |>
#   gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "Willapa Bay Natural") |>
#   gt::fmt_number(contains("Rhat"), decimals = 2) |>
#   gt::fmt_number(contains("n_eff"), decimals = 0)

#Willapa, all years, similar to "all pops"
oat |> 
  filter(StockID == 161) |> 
  # arrange(mod, year) |>
  # #filter(str_detect(mod, "stipm")) |> 
  # print(n = 100)
  group_by(mod) |> 
  summarise(
    n_diverg = max(n_diverg),
    Rhat_med = median(Rhat), Rhat_max = max(Rhat),
    n_eff_med = median(n_eff), n_eff_min = min(n_eff),
    .groups = "drop") |> 
  gt::gt(rowname_col = "mod") |> 
#  gt::cols_hide(c("pop_id", "pop")) |> 
  gt::tab_header(title = "One-ahead Rhat and effective draws", subtitle = "2009-19, Willapa Bay Natural") |> 
  gt::fmt_number(contains("Rhat"), decimals = 2) |> 
  gt::fmt_number(contains("n_eff"), decimals = 0)

# #n_eff boxes
# oat |> 
#   filter(StockID == 161, str_detect(mod, "stipm")) |> 
#   ggplot(aes(mod, n_eff)) + geom_boxplot() + geom_jitter(width = 0.1)
# #n_eff col timeseries, arguably slightly better after 2012
# oat |> 
#   filter(StockID == 161, str_detect(mod, "stipm")) |> 
#   ggplot(aes(factor(year), n_eff, fill = mod)) + geom_col(position = "dodge") + scale_fill_grey()

```

## One-ahead Performance

### Lag3 paneled by year

Posterior medians and 95% CIs are shown for the AR1 (purple) and STIPM (gold) for each individual forecast year relative to the filtered data subset (darker lines) and the subsequent return observations. Also shown are the deterministic lagged trailing mean (cyan) and the submitted forecast of record (pink).

```{r oat_obs_pred_patchwork, fig.width=8, fig.height=10, eval=FALSE}
set_names(2008:2018) |> 
  map(function(x) {
    d <- coho_data_tbl |> filter(StockID == 161, between(year, 1998, x)) |> select(year, StockID, pop, rtrn)

    oat_obs |> 
      filter(StockID == 161, str_detect(lag_mod, "l3|pre_fcst"), year == x+1) |> 
      ggplot() +
      #all pre-forecast data
      geom_line(data = d, aes(x = year, y = rtrn), color = grey(0.8), size = 0.9) +
      geom_point(data = d, aes(x = year, y = rtrn), color = grey(0.8), size = 0.9) +
      #available data for L3
      geom_line(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), color = grey(0.4), size = 0.7) +
      geom_point(data = d |> filter(year %in% as.character(1998:x-2)), aes(x = year, y = rtrn), color = grey(0.4), size = 0.7) +
      #observed in forecast year
      geom_point(aes(x = year, y = rtrn), color = 1, shape = 17)  +
      #forecasts
      geom_pointrange(aes(x = year, y = `50%`, ymin = `2.5%`, ymax = `97.5%`, color = lag_mod), fatten = 1.02, position = position_dodge(width = 0.5), show.legend = F) +
      geom_point(aes(x = year, y = `50%`, color = lag_mod), size = 0.9, position = position_dodge(width = 0.5), show.legend = F) +
      geom_vline(xintercept = x-2, linetype = "dotted", size = 1) +
      scale_x_continuous(name = "", limits = c(1998, 2020), breaks = 1998:2020, labels = 1998:2020) +
#      scale_y_continuous(name = "Return", limits = c(0,350000), labels = scales::comma) + #limits drops point error when outside ymax
      scale_y_continuous(name = "Return", labels = scales::comma) + #limits drops point error when outside ymax
      scale_color_manual(name = "", values = pal_lag_mod) +
      theme(axis.text = element_text(size = 6), axis.title = element_text(size = 5))

}) |> 
  patchwork::wrap_plots(ncol = 1)

ggsave("O:/code/coho/forecast_wb/f_oat_l3_by_pred_year.png", width = 7, height = 9)
```

### Lag3 50th with ribbons

The panels of years in the prior plot are collapsed, with ribbons illustrating the 50th and 95% CI. 

```{r l3_50th_ribbons, fig.width=8, fig.height=8, eval=FALSE}
oat_ribbon <- function(stk = 161, lg_md = "l3"){
  d <- oat_obs |> filter(StockID == stk, str_detect(lag_mod, lg_md))
  g <- ggplot(d, aes(year)) + 
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_point(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn)) +
    geom_ribbon(aes(ymin = `2.5%`, ymax = `97.5%`, color = lag_mod, fill = lag_mod), alpha = 0.2, linetype = 0) + 
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`, color = lag_mod, fill = lag_mod), alpha = 0.4, linetype = 0) + 
    geom_line(aes(y = `50%`, color = lag_mod), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2)) +
    scale_y_continuous(name = "Return", labels = scales::comma) +
    scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
    theme(legend.position = "top") +
    labs(title = d$pop[1])
  return(g)
}

g <- oat_ribbon(lg_md = "l3|pre_fcst")
g + facet_wrap(~lag_mod)

```

### Lag3 error time series

The raw prediction error (observed - forecast) is shown for the state space models alongside the deterministic mean and the recorded FRAM forecast.

```{r l3_error_series, fig.width=7, fig.height=7, eval=FALSE}
ggdata <- oat_obs |> 
  filter(StockID == 161, str_detect(lag_mod, "l3|pre_fcst")) |> 
  arrange(year, lag_mod) |> 
  mutate(year = as.character(year))

ggplot(ggdata, aes(year)) + 
  geom_col(aes(y = err, color = lag_mod, fill = lag_mod), position = position_dodge(), width = 0.7) +
  annotate("text", "2013", 8e4, label = "Underforecast:\n more returned than expected") +
  annotate("text", "2017", -5.5e4, label = "Overforecast:\n fewer returned than expected") +
  scale_x_discrete(name = "", guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(name = "Error: observed - forecast", labels = scales::comma) + #limits drops point error when outside ymax
  scale_color_manual(name = "", values = pal_lag_mod, na.translate = FALSE, aesthetics = c("color", "fill")) +
  theme(legend.position = "top")

ggsave("O:/code/coho/forecast_wb/f_oat_l3_error.png", width = 7, height = 5)

```

### Overlaid posterior medians by lag

```{r oat_lag_overlay, fig.width=7, fig.height=7, eval=FALSE}
oat_lag_overlay <- function(stk = 161){
  d <- oat_obs |> filter(StockID == stk, !str_detect(lag_mod, "pre_")) |> 
    mutate(
      lag = paste("Lag", str_sub(lag_mod, 2,2)),
      mod = toupper(str_sub(lag_mod, 4, 40))
    )
 
  ggplot(d, aes(year)) + 
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2008)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    # geom_col(data = coho_data_tbl |> filter(StockID == stk, between(year, 2009, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), fill = grey(0.6)) +
    geom_line(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_point(data = coho_data_tbl |> filter(StockID == stk, between(year, 1998, 2019)) |> select(year, StockID, pop, rtrn), aes(y = rtrn), color = grey(0.6)) +
    geom_line(aes(y = `50%`, color = lag), size = 1.2) +
    scale_x_continuous(name = "", breaks = 1998:2020, labels = 1998:2020, guide = guide_axis(n.dodge = 2), minor_breaks = 1998:2020) +
    scale_y_continuous(name = "Return", labels = scales::comma) + 
    scale_color_manual("", values = as.vector(wacolors::wa_pal("rainier", which = c("ground","winter_sky","paintbrush"))), aesthetics = c("color", "fill")) +
    facet_wrap(~mod, ncol = 1) + 
    theme(legend.position = "top") +
    labs(title = d$pop[1], color = "", fill = "")
  
}

oat_lag_overlay(161)

#ggsave("O:/code/coho/forecast_wb/f_oat_pred_lag_overlay.png", width = 8, height = 9)

oat_lag_overlay(131) + #Quill Fall
oat_lag_overlay(153) +  #Hump
oat_lag_overlay(149) + #Chehalis
oat_lag_overlay(157) + #GH Misc
oat_lag_overlay(63) + #Deschutes
oat_lag_overlay(69) + #Nisq
patchwork::plot_layout(ncol = 2)
```

### Summarized performance measures

```{r gt_perf_measures, eval=FALSE}
summary_start_year <- 2009

oat_obs |>
  filter(between(year, summary_start_year, 2019)) |> 
  arrange(year, lag_mod) |>
  group_by(StockID, pop, lag_mod) |> 
  summarise(
    msa = 100*(exp(median(abs(lar))) - 1),
    mase = 100*mean(ase),
    # mape = 100*mean(err_abs_pct),
    # rmse = sqrt(mean(err^2)),
    # # me = mean(err),
    # # mpe = 100*median(err_pct),
    # # nse_mod = 1 - ( sum(err_abs) / sum(abs(rtrn - mean(rtrn))) ),
    # # nse = 1 - ( sum(err^2) / sum((rtrn - mean(rtrn))^2) ),
    .groups = "drop") |> 
  mutate(
    lag = str_sub(lag_mod, 2,2),
    lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
    mod = toupper(str_sub(lag_mod, 4, 40)),
    mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) |> 
  filter(StockID == 161) |>
  gt::gt(rowname_col = "mod", groupname_col = "lag") |> 
  gt::cols_hide(c(StockID, pop, lag_mod)) |> 
  gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |> 
  # gt::fmt_percent(columns = c(msa, mase, mape), scale_values = F, decimals = 0) |> 
  # gt::fmt_number(columns = c(rmse), decimals = 0) |> 
  gt::data_color(
    columns = c(msa, mase), 
    colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
  ) |> 
  gt::tab_header(
    title = "One-ahead Performance Measures", 
    subtitle = paste(summary_start_year, "- 2019; AR1 and STIPM calculated from posterior median")) |> 
  gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
  gt::tab_source_note("Mean Abs. Scaled Error (MASE)") |> 
  # gt::tab_source_note("Mean Abs. Percent Error (MAPE)") |> 
  # gt::tab_source_note("Root Mean Square Error (RMSE)") |> 
  # #gt::tab_source_note("Mean Error (ME)")

gt::gtsave("O:/code/coho/forecast_wb/gt_oat_perf_measures_09_19.png", expand = 20)


perf_smry <- function(stk, summary_start_year = 2009){
  gt_data <- oat_obs |>
    filter(StockID == stk, between(year, summary_start_year, 2019)) |> 
    arrange(year, lag_mod) |>
    group_by(StockID, pop, lag_mod) |> 
    summarise(
      msa = 100*(exp(median(abs(lar))) - 1),
      mase = 100*mean(ase),
      .groups = "drop") |> 
    mutate(
      lag = str_sub(lag_mod, 2,2),
      lag = if_else(is.na(as.integer(lag)), "", paste("Lag", lag)),
      mod = toupper(str_sub(lag_mod, 4, 40)),
      mod = if_else(mod == "_FCST", "Previous FRAM forecast", mod)
    ) 
  
   gt_data |> 
    gt::gt(rowname_col = "mod", groupname_col = "lag") |>
    gt::cols_hide(c(StockID, pop, lag_mod)) |>
    gt::fmt_percent(columns = c(msa, mase), scale_values = F, decimals = 0) |>
    gt::data_color(
      columns = c(msa, mase),
      colors = scales::col_numeric(palette = c("white", "orange", "red"), domain = c(0,200), na.color = "red")
    ) |>
    gt::tab_header(
      title = paste("One-ahead Performance:", gt_data[1,"pop"], gt_data[1,"StockID"]),
      subtitle = paste(summary_start_year, "- 2019; AR1 and STIPM calculated from posterior median")) |>
    gt::tab_source_note("Median Symmetric Accuracy (MSA)") |>
    gt::tab_source_note("Mean Abs. Scaled Error (MASE)")
}

perf_smry(161)

# coho_data_tbl |> distinct(StockID) |> pluck("StockID") |> 
#   walk(~perf_smry(.x) |> gt::gtsave(paste("O:/code/coho/forecast_wb/gt_oat_summary/gt_oat_perf_measures_stk",.x, ".png"), expand = 20))

```

# Full dataset

This chunk performs the actual fits, with the final set of commented lines left for convenience when running more iterations and chains on AWS.

```{r full_data_fit}
#coho_data_tbl <- coho_data_tbl_prelim

#use all data
(
  stan_data_list <- stan_data_filter(
    coho_data_tbl,
    data_year_min = 1998, data_year_max = 2022, 
    lag_smolt = 0, lag_MS = 0, lag_spwn = 0, lag_hvst = 0)
)

# print(stan_data_list$y_min_max)
# print(c("smolt", range(stan_data_list$smolt$year)))
# print(c("MS", range(stan_data_list$MS$year)))
# print(c("spwn", range(stan_data_list$spwn$year)))
# print(c("hvst", range(stan_data_list$hvst$year)))
# print(c("rtrn", range(stan_data_list$rtrn$year)))

# #with minimal defaults to test that sampling is working
# fit_ar1 <- stan_ar1(stan_data = stan_data_list)
# fit_stipm <- stan_stipm(stan_data = stan_data_list)

# #as 'minimal but okay' for local machine runs
# fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = 500, n_chain = 4)
# fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = 500, n_chain = 4)

# #as 'objects are getting pretty big' runs on AWS
# fit_stipm <- stan_stipm(stan_data = stan_data_list, n_iter = 2000, n_chain = 4)
# saveRDS(fit_stipm, "fit_stipm_2023.rds")

# fit_ar1 <- stan_ar1(stan_data = stan_data_list, n_iter = 2000, n_chain = 4)
# saveRDS(fit_ar1, "fit_ar1_2023.rds")


##can construct estimates beyond desired forecast year
##by "tricking" the wrapper that sets n_year
##then retain fit objects and extract prediction
##as adult_est (for AR1) and adult_est+harvest_est (for STIPM)
# stan_data_list$y_min_max <- 1998:2030

```

## Output objects

This chunk extracts adult estimates and is somewhat superseded by `pdrw` object below, but left in case. The first block extracts draws in a `tidybayes` flow while the second uses the `rstan` basic `summary()$summary` mode.

```{r read_full_data_fits, eval=FALSE}
# #these are huge, balloons memory
# #per year have rows = 4chains * post-warmup
# 
# fit_ar1 <- readRDS("~/O/code/coho/forecast_wb/fit_ar1_2023.rds")
# fit_stipm <- readRDS("~/O/code/coho/forecast_wb/fit_stipm_2023.rds")


# fit_ar1_wb <- spread_draws(fit_ar1, adult_est[n_year,n_pop] | n_pop) |>
#   ungroup() |> select(n_year, .iteration, rtrn = `34`) |>
#   mutate(year = first(stan_data_list$y_min_max)-1+n_year, mod = "ar1")
#  
# # saveRDS(fit_ar1_wb, "fit_ar1_wb_adult_est.rds")
#  
# # #stipm requires constructing total return from harvest + spawning adults
# fit_stipm_wb <- full_join(
#   spread_draws(fit_stipm, adult_est[n_year,n_pop] | n_pop) |>
#     ungroup() |> select(n_year, .chain, .iteration, .draw, spwn = `34`)
#   ,
#   spread_draws(fit_stipm, harvest_est[n_year,n_pop] | n_pop) |>
#     ungroup() |> select(n_year, .chain, .iteration, .draw, hvst = `34`)
#   ,
#   by = c("n_year", ".chain", ".iteration", ".draw")) |>
#   mutate(year = first(stan_data_list$y_min_max)-1+n_year, rtrn = spwn+hvst, mod = "stipm") |> 
#   bind_rows(
#     spread_draws(fit_stipm, adult_pred[n_pop] | n_pop) |> ungroup() |> 
#       select(.chain, .iteration, .draw, rtrn = `34`) |> 
#       mutate(year = last(stan_data_list$y_min_max)+1, mod = "stipm")
#   )
#  
# # saveRDS(fit_stipm_wb, "fit_stipm_wb_adult_est.rds")


#rstan summary()$summary 
fit_ar1_pred <- summary(fit_ar1, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
    mod = "ar1",
    n_diverg = get_sampler_params(fit_ar1, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")
#recall stipm adult_pred is smolt[n_year]*surv[n_year] to give adult run size (spwn+hvst) n_year+1
fit_stipm_pred <- summary(fit_stipm, pars = "adult_pred")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    pop_id = as.numeric(str_extract(stan_out, "[0-9]+")),
    year = last(stan_data_list$y_min_max) + 1, #pred year, max data year x+1
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")


(g <- bind_rows(fit_ar1_pred, fit_stipm_pred) |> 
  filter(StockID >= 130) |> 
  arrange(StockID) |> 
  select(mod, StockID, StockLongName, pop_id,
         `2.5%`,`25%`,`50%`,`75%`,`97.5%`,
         n_eff, Rhat, n_diverg) |> 
  unite("stk", StockID, StockLongName, pop_id) |> 
  gt(groupname_col = "stk") |> 
  tab_header(title = "2023 forecast return estimates", 
             subtitle = "Posterior quantiles and stan diagnostics") |> 
  fmt_integer(columns = c("n_eff", `2.5%`,`25%`,`50%`,`75%`,`97.5%`)) |> 
  fmt_number(columns = "Rhat", decimals = 3) |> 
  tab_style(locations = cells_body(columns = `50%`), style = cell_fill("grey80")) |> 
  tab_style(locations = cells_body(columns = `2.5%`), style = cell_fill("#F5E298")) |> 
  tab_style(locations = cells_body(columns = `97.5%`), style = cell_fill("#C6F5BA"))
)

#gtsave(g, "2023_select_coastal_WA.png", vwidth = 1000, vheight = 900, expand = 10)
```

This chunk extracts the posterior distributions of per-year, per-stock return estimates for both the AR1 and STIPM model fits.

```{r pdrw, eval=FALSE}
pdrw <- bind_rows(
  #stipm
  #pre-fcst year estimate series
  full_join(
    spread_draws(fit_stipm, adult_est[n_year,n_pop] | n_pop) |> ungroup() |> 
      pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "spwn"),
    spread_draws(fit_stipm, harvest_est[n_year,n_pop] | n_pop) |> ungroup() |> 
      pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "hvst"),
    by = c("n_year", ".chain", ".iteration", ".draw", "pop_id")
  ) |>
    mutate(
      year = first(stan_data_list$y_min_max)-1+n_year, n_year = NULL,
      rtrn = spwn+hvst,
      mod = "stipm") |> 
    #add fcst year
    bind_rows(
      spread_draws(fit_stipm, adult_pred[n_pop] | n_pop) |> ungroup() |> 
        pivot_longer(cols = -c(.chain, .iteration, .draw), names_to = "pop_id", values_to = "rtrn") |> 
        mutate(year = last(stan_data_list$y_min_max)+1, mod = "stipm")
    )
  ,
  #ar1
  spread_draws(fit_ar1, adult_est[n_year,n_pop] | n_pop) |> ungroup() |>
    pivot_longer(cols = -c(n_year, .chain, .iteration, .draw), names_to = "pop_id", values_to = "rtrn") |> 
    mutate(
      year = first(stan_data_list$y_min_max)-1+n_year, n_year = NULL,
      mod = "ar1")  
  ) |> 
  mutate(pop_id = as.numeric(pop_id)) |> 
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

# #from AWS
# saveRDS(pdrw, "pdrw_2023.rds")
```

## Return plots

```{r posterior_density}
pdrw |> 
  filter(StockID == 161, year == 2023) |> 
  ggplot(aes(x = rtrn, color = mod, fill = mod)) + 
  stat_slab(alpha = 0.5) +
  geom_vline(
    data = filter(pdrw, StockID==161, year == 2023) |> group_by(mod) |> 
      summarise(across(rtrn, median), .groups = "drop"),
    aes(xintercept = rtrn, color = mod), linewidth = 1.2,
    show.legend = F
  ) +
  geom_text(
    data = filter(pdrw, StockID==161, year == 2023) |> group_by(mod) |> 
      summarise(across(rtrn, median), .groups = "drop"),
    aes(x = rtrn, y = 1, label = scales::comma(round(rtrn)), color = mod),
    nudge_x = c(-12000, 12000),
    nudge_y = c(-0.02, 0.02),
    show.legend = F
  ) +
  scale_y_continuous("") +
  scale_x_continuous("Estimated return", labels = scales::comma) +
  scale_fill_discrete(type = pal_lag_mod[1:2], aesthetics = c("color", "fill")) +
  theme(legend.position = "top", legend.title = element_blank())
```

This chunk defines a convenience plotting function taking a *StockID* argument and returning time series plots paneled by model of the posterior distribution of estimated total return.

```{r ggfcst_ts}
ggfcst_ts <- function(stk = 161, year_pred = 2023) {
  pdrw_stk <- filter(pdrw, StockID == stk, year >= 1998)
  
  # #could add logic for function arg ar1only = T 
  # pdrw_stk <- filter(pdrw_stk, mod == "ar1") 
  
  ggplot(pdrw_stk, aes(year, rtrn)) +
    stat_lineribbon(aes(fill = mod), alpha = 0.4, show.legend = F) +
    geom_point(data = filter(stan_data_list$rtrn, StockID==stk), color = 1) +
    geom_text(
      data = pdrw_stk |> 
        filter(year == year_pred) |> 
        group_by(mod, year) |> 
        summarise(rtrn = round(median(rtrn)), .groups = "drop"),
      aes(label = scales::comma(rtrn)),
      nudge_x = 1.5) +
    facet_wrap(~mod) +
    scale_fill_discrete(type = pal_lag_mod[1:2]) +
    scale_x_continuous("", breaks = seq(1998, year_pred, by = 2),
                       guide = guide_axis(angle = 45)) +
    scale_y_continuous("Forecast return", labels = scales::comma) +
    labs(title = paste(pdrw_stk$StockLongName[1], year_pred,"forecast"), 
         subtitle = "Posterior intervals of fits to data since 1998",
         fill = "Model") +
    theme_light(base_size = 13)
}

ggfcst_ts()
```

```{r other_stocks, eval=FALSE}
ggfcst_ts_coast <- map(
  coho_data_tbl |> distinct(StockID, StockLongName) |> filter(StockID > 120) |> pluck("StockID") |> set_names(), 
 ~ggfcst_ts(.x))

patchwork::wrap_plots(ggfcst_ts_coast)

ggfcst_ts(135) #Hoh
ggfcst_ts(131) #Quill Fall

ggfcst_ts_hc <- map(c(45, 51, 55, 59) |> set_names(), ~ggfcst_ts(.x))
patchwork::wrap_plots(ggfcst_ts_hc[c(1,3,4)], ncol = 1)

ggfcst_ts_jdf <- map(c(107, 111, 115, 117) |> set_names(), ~ggfcst_ts(.x))
patchwork::wrap_plots(ggfcst_ts_jdf)

ggfcst_ts_stsno <- map(c(29, 35) |> set_names(), ~ggfcst_ts(.x))
patchwork::wrap_plots(ggfcst_ts_stsno, ncol = 1)

bind_rows(fit_ar1_pred, fit_stipm_pred) |> 
  filter(StockID %in% c(29)) |> 
  select(year, mod, StockID, StockLongName, pop_id, `2.5%`,`25%`,`50%`,`75%`,`97.5%`) |> 
  pivot_wider(names_from = mod, values_from = `2.5%`:`97.5%`)

```

## Weighting

```{r simple_msa_weighted_median, eval=TRUE, results='show'}
#stack weighting for 2023...
#using weighting eqn in slide 31, with Oct 2021 lag-1 MSA

pdrw |>
  filter(StockID==161, year == 2023) |>
  group_by(mod) |>
  summarise(across(rtrn, median), .groups = "drop") |>
  mutate(
    w = 1/c(ar1 = 0.96, stipm = 0.65) / sum(1/c(ar1 = 0.96, stipm = 0.65)),
    rtrn_w = rtrn * w
  ) |> 
  summarise(oa3 = sum(rtrn_w), ja3 = oa3 * 1.2315) |> 
  mutate(across(everything(), ~scales::comma(round(.)))) |> 
  gt::gt()
  
```


## extract MS

```{r eval=FALSE}
# boot::inv.logit()
# or: function(x) { exp(x)/(1+exp(x)) }

# summary(fit_stipm, pars = "mu_mu_surv")$summary |>
#   as.data.frame() |> rownames_to_column("stan_out") |> tibble()

ms <- summary(fit_stipm, pars = "logit_smolt_survival_pop")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    across(`2.5%`:`97.5%`, ~(exp(.)/(1+exp(.)))),
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    yr = str_remove_all(stan_out, "logit_smolt_survival_pop\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[1])),
    year = yr + first(stan_data_list$y_min_max) - 1,
    pop_id = str_remove_all(stan_out, "logit_smolt_survival_pop\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[2])),
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

patchwork::wrap_plots(list(
  
ms |> filter(StockID==161) |> ggplot(aes(year, `50%`)) + geom_line() + geom_point() + geom_smooth(se = F) + scale_y_continuous("median posterior marine survival", labels = scales::percent) + labs(subtitle = "WB nat")

,
ms |> filter(StockID==161) |> 
  select(StockID, StockLongName, year, `2.5%`:`97.5%`) |> 
  pivot_longer(names_to = "q", values_to = "est", `2.5%`:`97.5%`) |>  
  ggplot(aes(q, est, color = q)) + 
  geom_boxplot(aes(fill = q), alpha = 0.4) + scale_y_continuous("median posterior marine survival", labels = scales::percent)
))

#EJDF and WJDF
ms |> filter(StockID %in% c(115,117)) |> ggplot(aes(year, `50%`, color = StockLongName)) + geom_line() + geom_point() + geom_smooth(se = F) + scale_y_continuous("median posterior marine survival", labels = scales::percent)

#Queets
ms |> filter(StockID==139) |> ggplot(aes(year, `50%`)) + geom_line() + geom_point() + geom_smooth(se = F) + scale_y_continuous("median posterior marine survival", labels = scales::percent) + labs(subtitle = "Queets nat")
```

```{r eval=FALSE}
#Coastal per requests, export "for consideration, use at your own risk" 
list(
  marine_surv = ms |> 
    filter(StockID > 125) |> 
    group_by(StockID, StockLongName, year) |> 
    summarise(across(c(n_eff, Rhat, `2.5%`:`97.5%`), median), .groups = "drop")
  ,
  rtrn = pdrw |> 
    filter(StockID > 125) |> 
    group_by(mod, StockID, StockLongName, year) |> 
    summarise(across(rtrn, median), .groups = "drop")
) |>
  writexl::write_xlsx("T:/DFW-Salmon Mgmt Modeling Team - General/Preseason/Coho/2022/forecasts/stipm_prelim_2022_estimates.xlsx")


ms |> 
  filter(StockID > 125) |> 
  ggplot(aes(year, `50%`, color = StockLongName)) + 
  geom_line(show.legend = T) + geom_point(show.legend = F) + geom_smooth(se = F, show.legend = F) + geom_hline(yintercept = 0.085) + 
  scale_y_continuous("median posterior marine survival", labels = scales::percent) + 
  #facet_wrap(~StockLongName, ncol = 1) +
  theme(text = element_text(size = 20))

```

## extract smolts

```{r eval=FALSE}
smolt <- summary(fit_stipm, pars = "smolt_est")$summary |>
  as.data.frame() |> rownames_to_column("stan_out") |> tibble() |>
  mutate(
    #across(`2.5%`:`97.5%`, ~(exp(.)/(1+exp(.)))),
    var = str_extract(stan_out, "[a-z]+[:punct:][a-z]+"),
    yr = str_remove_all(stan_out, "smolt_est\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[1])),
    year = yr + first(stan_data_list$y_min_max) - 1,
    pop_id = str_remove_all(stan_out, "smolt_est\\[|\\]") |> str_split(",") |> map_dbl(~as.numeric(.x[2])),
    mod = "stipm",
    n_diverg = get_sampler_params(fit_stipm, inc_warmup = FALSE) |>
      map_dbl(~.x[, "divergent__"] |> sum()) |> sum()
  ) |>
  left_join(distinct(coho_data_tbl, pop_id, StockID, StockLongName), by = "pop_id")

smolt |> filter(StockID == 135)

```

